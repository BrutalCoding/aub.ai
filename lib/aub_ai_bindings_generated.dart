// ignore_for_file: always_specify_types
// ignore_for_file: camel_case_types
// ignore_for_file: non_constant_identifier_names
// ignore_for_file: deprecated_member_use
// ignore_for_file: unused_field

// AUTO GENERATED FILE, DO NOT EDIT.
//
// Generated by `package:ffigen`.
// ignore_for_file: type=lint
import 'dart:ffi' as ffi;

/// Bindings for `src/aub_ai.h`.
///
/// Regenerate bindings with `dart run ffigen --config ffigen.yaml`.
///
class AubAiBindings {
  /// Holds the symbol lookup function.
  final ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName)
      _lookup;

  /// The symbols are looked up in [dynamicLibrary].
  AubAiBindings(ffi.DynamicLibrary dynamicLibrary)
      : _lookup = dynamicLibrary.lookup;

  /// The symbols are looked up with [lookup].
  AubAiBindings.fromLookup(
      ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName)
          lookup)
      : _lookup = lookup;

  /// Helpers for getting default parameters
  llama_model_params llama_model_default_params() {
    return _llama_model_default_params();
  }

  late final _llama_model_default_paramsPtr =
      _lookup<ffi.NativeFunction<llama_model_params Function()>>(
          'llama_model_default_params');
  late final _llama_model_default_params = _llama_model_default_paramsPtr
      .asFunction<llama_model_params Function()>();

  llama_context_params llama_context_default_params() {
    return _llama_context_default_params();
  }

  late final _llama_context_default_paramsPtr =
      _lookup<ffi.NativeFunction<llama_context_params Function()>>(
          'llama_context_default_params');
  late final _llama_context_default_params = _llama_context_default_paramsPtr
      .asFunction<llama_context_params Function()>();

  llama_model_quantize_params llama_model_quantize_default_params() {
    return _llama_model_quantize_default_params();
  }

  late final _llama_model_quantize_default_paramsPtr =
      _lookup<ffi.NativeFunction<llama_model_quantize_params Function()>>(
          'llama_model_quantize_default_params');
  late final _llama_model_quantize_default_params =
      _llama_model_quantize_default_paramsPtr
          .asFunction<llama_model_quantize_params Function()>();

  /// Initialize the llama + ggml backend
  /// If numa is true, use NUMA optimizations
  /// Call once at the start of the program
  void llama_backend_init() {
    return _llama_backend_init();
  }

  late final _llama_backend_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('llama_backend_init');
  late final _llama_backend_init =
      _llama_backend_initPtr.asFunction<void Function()>();

  /// optional:
  void llama_numa_init(
    int numa,
  ) {
    return _llama_numa_init(
      numa,
    );
  }

  late final _llama_numa_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Int32)>>(
          'llama_numa_init');
  late final _llama_numa_init =
      _llama_numa_initPtr.asFunction<void Function(int)>();

  /// Call once at the end of the program - currently only used for MPI
  void llama_backend_free() {
    return _llama_backend_free();
  }

  late final _llama_backend_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('llama_backend_free');
  late final _llama_backend_free =
      _llama_backend_freePtr.asFunction<void Function()>();

  ffi.Pointer<llama_model> llama_load_model_from_file(
    ffi.Pointer<ffi.Char> path_model,
    llama_model_params params,
  ) {
    return _llama_load_model_from_file(
      path_model,
      params,
    );
  }

  late final _llama_load_model_from_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_model> Function(ffi.Pointer<ffi.Char>,
              llama_model_params)>>('llama_load_model_from_file');
  late final _llama_load_model_from_file =
      _llama_load_model_from_filePtr.asFunction<
          ffi.Pointer<llama_model> Function(
              ffi.Pointer<ffi.Char>, llama_model_params)>();

  void llama_free_model(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_free_model(
      model,
    );
  }

  late final _llama_free_modelPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_model>)>>(
          'llama_free_model');
  late final _llama_free_model = _llama_free_modelPtr
      .asFunction<void Function(ffi.Pointer<llama_model>)>();

  ffi.Pointer<llama_context> llama_new_context_with_model(
    ffi.Pointer<llama_model> model,
    llama_context_params params,
  ) {
    return _llama_new_context_with_model(
      model,
      params,
    );
  }

  late final _llama_new_context_with_modelPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_context> Function(ffi.Pointer<llama_model>,
              llama_context_params)>>('llama_new_context_with_model');
  late final _llama_new_context_with_model =
      _llama_new_context_with_modelPtr.asFunction<
          ffi.Pointer<llama_context> Function(
              ffi.Pointer<llama_model>, llama_context_params)>();

  /// Frees all allocated memory
  void llama_free(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_free(
      ctx,
    );
  }

  late final _llama_freePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_free');
  late final _llama_free =
      _llama_freePtr.asFunction<void Function(ffi.Pointer<llama_context>)>();

  int llama_time_us() {
    return _llama_time_us();
  }

  late final _llama_time_usPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function()>>('llama_time_us');
  late final _llama_time_us = _llama_time_usPtr.asFunction<int Function()>();

  int llama_max_devices() {
    return _llama_max_devices();
  }

  late final _llama_max_devicesPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function()>>('llama_max_devices');
  late final _llama_max_devices =
      _llama_max_devicesPtr.asFunction<int Function()>();

  bool llama_supports_mmap() {
    return _llama_supports_mmap();
  }

  late final _llama_supports_mmapPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>('llama_supports_mmap');
  late final _llama_supports_mmap =
      _llama_supports_mmapPtr.asFunction<bool Function()>();

  bool llama_supports_mlock() {
    return _llama_supports_mlock();
  }

  late final _llama_supports_mlockPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>('llama_supports_mlock');
  late final _llama_supports_mlock =
      _llama_supports_mlockPtr.asFunction<bool Function()>();

  bool llama_supports_gpu_offload() {
    return _llama_supports_gpu_offload();
  }

  late final _llama_supports_gpu_offloadPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>(
          'llama_supports_gpu_offload');
  late final _llama_supports_gpu_offload =
      _llama_supports_gpu_offloadPtr.asFunction<bool Function()>();

  ffi.Pointer<llama_model> llama_get_model(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_get_model(
      ctx,
    );
  }

  late final _llama_get_modelPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_model> Function(
              ffi.Pointer<llama_context>)>>('llama_get_model');
  late final _llama_get_model = _llama_get_modelPtr.asFunction<
      ffi.Pointer<llama_model> Function(ffi.Pointer<llama_context>)>();

  int llama_n_ctx(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_n_ctx(
      ctx,
    );
  }

  late final _llama_n_ctxPtr = _lookup<
          ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<llama_context>)>>(
      'llama_n_ctx');
  late final _llama_n_ctx =
      _llama_n_ctxPtr.asFunction<int Function(ffi.Pointer<llama_context>)>();

  int llama_n_batch(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_n_batch(
      ctx,
    );
  }

  late final _llama_n_batchPtr = _lookup<
          ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<llama_context>)>>(
      'llama_n_batch');
  late final _llama_n_batch =
      _llama_n_batchPtr.asFunction<int Function(ffi.Pointer<llama_context>)>();

  int llama_vocab_type1(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_vocab_type1(
      model,
    );
  }

  late final _llama_vocab_type1Ptr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_vocab_type');
  late final _llama_vocab_type1 = _llama_vocab_type1Ptr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_rope_type1(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_rope_type1(
      model,
    );
  }

  late final _llama_rope_type1Ptr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_rope_type');
  late final _llama_rope_type1 =
      _llama_rope_type1Ptr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_n_vocab(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_n_vocab(
      model,
    );
  }

  late final _llama_n_vocabPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_n_vocab');
  late final _llama_n_vocab =
      _llama_n_vocabPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_n_ctx_train(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_n_ctx_train(
      model,
    );
  }

  late final _llama_n_ctx_trainPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_n_ctx_train');
  late final _llama_n_ctx_train = _llama_n_ctx_trainPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_n_embd(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_n_embd(
      model,
    );
  }

  late final _llama_n_embdPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_n_embd');
  late final _llama_n_embd =
      _llama_n_embdPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Get the model's RoPE frequency scaling factor
  double llama_rope_freq_scale_train(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_rope_freq_scale_train(
      model,
    );
  }

  late final _llama_rope_freq_scale_trainPtr =
      _lookup<ffi.NativeFunction<ffi.Float Function(ffi.Pointer<llama_model>)>>(
          'llama_rope_freq_scale_train');
  late final _llama_rope_freq_scale_train = _llama_rope_freq_scale_trainPtr
      .asFunction<double Function(ffi.Pointer<llama_model>)>();

  /// Get metadata value as a string by key name
  int llama_model_meta_val_str(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> key,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_model_meta_val_str(
      model,
      key,
      buf,
      buf_size,
    );
  }

  late final _llama_model_meta_val_strPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Char>, ffi.Size)>>('llama_model_meta_val_str');
  late final _llama_model_meta_val_str =
      _llama_model_meta_val_strPtr.asFunction<
          int Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Char>, int)>();

  /// Get the number of metadata key/value pairs
  int llama_model_meta_count(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_model_meta_count(
      model,
    );
  }

  late final _llama_model_meta_countPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_model_meta_count');
  late final _llama_model_meta_count = _llama_model_meta_countPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Get metadata key name by index
  int llama_model_meta_key_by_index(
    ffi.Pointer<llama_model> model,
    int i,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_model_meta_key_by_index(
      model,
      i,
      buf,
      buf_size,
    );
  }

  late final _llama_model_meta_key_by_indexPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_model>,
              ffi.Int32,
              ffi.Pointer<ffi.Char>,
              ffi.Size)>>('llama_model_meta_key_by_index');
  late final _llama_model_meta_key_by_index =
      _llama_model_meta_key_by_indexPtr.asFunction<
          int Function(
              ffi.Pointer<llama_model>, int, ffi.Pointer<ffi.Char>, int)>();

  /// Get metadata value as a string by index
  int llama_model_meta_val_str_by_index(
    ffi.Pointer<llama_model> model,
    int i,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_model_meta_val_str_by_index(
      model,
      i,
      buf,
      buf_size,
    );
  }

  late final _llama_model_meta_val_str_by_indexPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_model>,
              ffi.Int32,
              ffi.Pointer<ffi.Char>,
              ffi.Size)>>('llama_model_meta_val_str_by_index');
  late final _llama_model_meta_val_str_by_index =
      _llama_model_meta_val_str_by_indexPtr.asFunction<
          int Function(
              ffi.Pointer<llama_model>, int, ffi.Pointer<ffi.Char>, int)>();

  /// Get a string describing the model type
  int llama_model_desc(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_model_desc(
      model,
      buf,
      buf_size,
    );
  }

  late final _llama_model_descPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>,
              ffi.Size)>>('llama_model_desc');
  late final _llama_model_desc = _llama_model_descPtr.asFunction<
      int Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>, int)>();

  /// Returns the total size of all the tensors in the model in bytes
  int llama_model_size(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_model_size(
      model,
    );
  }

  late final _llama_model_sizePtr = _lookup<
          ffi.NativeFunction<ffi.Uint64 Function(ffi.Pointer<llama_model>)>>(
      'llama_model_size');
  late final _llama_model_size =
      _llama_model_sizePtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Returns the total number of parameters in the model
  int llama_model_n_params(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_model_n_params(
      model,
    );
  }

  late final _llama_model_n_paramsPtr = _lookup<
          ffi.NativeFunction<ffi.Uint64 Function(ffi.Pointer<llama_model>)>>(
      'llama_model_n_params');
  late final _llama_model_n_params = _llama_model_n_paramsPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Get a llama model tensor
  ffi.Pointer<ggml_tensor> llama_get_model_tensor(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> name,
  ) {
    return _llama_get_model_tensor(
      model,
      name,
    );
  }

  late final _llama_get_model_tensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<llama_model>,
              ffi.Pointer<ffi.Char>)>>('llama_get_model_tensor');
  late final _llama_get_model_tensor = _llama_get_model_tensorPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>)>();

  /// Returns 0 on success
  int llama_model_quantize(
    ffi.Pointer<ffi.Char> fname_inp,
    ffi.Pointer<ffi.Char> fname_out,
    ffi.Pointer<llama_model_quantize_params> params,
  ) {
    return _llama_model_quantize(
      fname_inp,
      fname_out,
      params,
    );
  }

  late final _llama_model_quantizePtr = _lookup<
          ffi.NativeFunction<
              ffi.Uint32 Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>,
                  ffi.Pointer<llama_model_quantize_params>)>>(
      'llama_model_quantize');
  late final _llama_model_quantize = _llama_model_quantizePtr.asFunction<
      int Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<llama_model_quantize_params>)>();

  /// Apply a LoRA adapter to a loaded model
  /// path_base_model is the path to a higher quality model to use as a base for
  /// the layers modified by the adapter. Can be NULL to use the current loaded model.
  /// The model needs to be reloaded before applying a new adapter, otherwise the adapter
  /// will be applied on top of the previous one
  /// Returns 0 on success
  int llama_model_apply_lora_from_file(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> path_lora,
    double scale,
    ffi.Pointer<ffi.Char> path_base_model,
    int n_threads,
  ) {
    return _llama_model_apply_lora_from_file(
      model,
      path_lora,
      scale,
      path_base_model,
      n_threads,
    );
  }

  late final _llama_model_apply_lora_from_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_model>,
              ffi.Pointer<ffi.Char>,
              ffi.Float,
              ffi.Pointer<ffi.Char>,
              ffi.Int32)>>('llama_model_apply_lora_from_file');
  late final _llama_model_apply_lora_from_file =
      _llama_model_apply_lora_from_filePtr.asFunction<
          int Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>, double,
              ffi.Pointer<ffi.Char>, int)>();

  /// Create an empty KV cache view. (use only for debugging purposes)
  llama_kv_cache_view llama_kv_cache_view_init(
    ffi.Pointer<llama_context> ctx,
    int n_max_seq,
  ) {
    return _llama_kv_cache_view_init(
      ctx,
      n_max_seq,
    );
  }

  late final _llama_kv_cache_view_initPtr = _lookup<
      ffi.NativeFunction<
          llama_kv_cache_view Function(ffi.Pointer<llama_context>,
              ffi.Int32)>>('llama_kv_cache_view_init');
  late final _llama_kv_cache_view_init =
      _llama_kv_cache_view_initPtr.asFunction<
          llama_kv_cache_view Function(ffi.Pointer<llama_context>, int)>();

  /// Free a KV cache view. (use only for debugging purposes)
  void llama_kv_cache_view_free(
    ffi.Pointer<llama_kv_cache_view> view,
  ) {
    return _llama_kv_cache_view_free(
      view,
    );
  }

  late final _llama_kv_cache_view_freePtr = _lookup<
          ffi
          .NativeFunction<ffi.Void Function(ffi.Pointer<llama_kv_cache_view>)>>(
      'llama_kv_cache_view_free');
  late final _llama_kv_cache_view_free = _llama_kv_cache_view_freePtr
      .asFunction<void Function(ffi.Pointer<llama_kv_cache_view>)>();

  /// Update the KV cache view structure with the current state of the KV cache. (use only for debugging purposes)
  void llama_kv_cache_view_update(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_kv_cache_view> view,
  ) {
    return _llama_kv_cache_view_update(
      ctx,
      view,
    );
  }

  late final _llama_kv_cache_view_updatePtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>,
              ffi.Pointer<llama_kv_cache_view>)>>('llama_kv_cache_view_update');
  late final _llama_kv_cache_view_update =
      _llama_kv_cache_view_updatePtr.asFunction<
          void Function(
              ffi.Pointer<llama_context>, ffi.Pointer<llama_kv_cache_view>)>();

  /// Returns the number of tokens in the KV cache (slow, use only for debug)
  /// If a KV cell has multiple sequences assigned to it, it will be counted multiple times
  int llama_get_kv_cache_token_count(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_get_kv_cache_token_count(
      ctx,
    );
  }

  late final _llama_get_kv_cache_token_countPtr = _lookup<
          ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_context>)>>(
      'llama_get_kv_cache_token_count');
  late final _llama_get_kv_cache_token_count =
      _llama_get_kv_cache_token_countPtr
          .asFunction<int Function(ffi.Pointer<llama_context>)>();

  /// Returns the number of used KV cells (i.e. have at least one sequence assigned to them)
  int llama_get_kv_cache_used_cells(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_get_kv_cache_used_cells(
      ctx,
    );
  }

  late final _llama_get_kv_cache_used_cellsPtr = _lookup<
          ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_context>)>>(
      'llama_get_kv_cache_used_cells');
  late final _llama_get_kv_cache_used_cells = _llama_get_kv_cache_used_cellsPtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  /// Clear the KV cache
  void llama_kv_cache_clear(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_kv_cache_clear(
      ctx,
    );
  }

  late final _llama_kv_cache_clearPtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_kv_cache_clear');
  late final _llama_kv_cache_clear = _llama_kv_cache_clearPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// Removes all tokens that belong to the specified sequence and have positions in [p0, p1)
  /// seq_id < 0 : match any sequence
  /// p0 < 0     : [0,  p1]
  /// p1 < 0     : [p0, inf)
  void llama_kv_cache_seq_rm(
    ffi.Pointer<llama_context> ctx,
    int seq_id,
    int p0,
    int p1,
  ) {
    return _llama_kv_cache_seq_rm(
      ctx,
      seq_id,
      p0,
      p1,
    );
  }

  late final _llama_kv_cache_seq_rmPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id, llama_pos,
              llama_pos)>>('llama_kv_cache_seq_rm');
  late final _llama_kv_cache_seq_rm = _llama_kv_cache_seq_rmPtr
      .asFunction<void Function(ffi.Pointer<llama_context>, int, int, int)>();

  /// Copy all tokens that belong to the specified sequence to another sequence
  /// Note that this does not allocate extra KV cache memory - it simply assigns the tokens to the new sequence
  /// p0 < 0 : [0,  p1]
  /// p1 < 0 : [p0, inf)
  void llama_kv_cache_seq_cp(
    ffi.Pointer<llama_context> ctx,
    int seq_id_src,
    int seq_id_dst,
    int p0,
    int p1,
  ) {
    return _llama_kv_cache_seq_cp(
      ctx,
      seq_id_src,
      seq_id_dst,
      p0,
      p1,
    );
  }

  late final _llama_kv_cache_seq_cpPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id,
              llama_seq_id, llama_pos, llama_pos)>>('llama_kv_cache_seq_cp');
  late final _llama_kv_cache_seq_cp = _llama_kv_cache_seq_cpPtr.asFunction<
      void Function(ffi.Pointer<llama_context>, int, int, int, int)>();

  /// Removes all tokens that do not belong to the specified sequence
  void llama_kv_cache_seq_keep(
    ffi.Pointer<llama_context> ctx,
    int seq_id,
  ) {
    return _llama_kv_cache_seq_keep(
      ctx,
      seq_id,
    );
  }

  late final _llama_kv_cache_seq_keepPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>,
              llama_seq_id)>>('llama_kv_cache_seq_keep');
  late final _llama_kv_cache_seq_keep = _llama_kv_cache_seq_keepPtr
      .asFunction<void Function(ffi.Pointer<llama_context>, int)>();

  /// Adds relative position "delta" to all tokens that belong to the specified sequence and have positions in [p0, p1)
  /// If the KV cache is RoPEd, the KV data is updated accordingly:
  /// - lazily on next llama_decode()
  /// - explicitly with llama_kv_cache_update()
  /// p0 < 0 : [0,  p1]
  /// p1 < 0 : [p0, inf)
  void llama_kv_cache_seq_add(
    ffi.Pointer<llama_context> ctx,
    int seq_id,
    int p0,
    int p1,
    int delta,
  ) {
    return _llama_kv_cache_seq_add(
      ctx,
      seq_id,
      p0,
      p1,
      delta,
    );
  }

  late final _llama_kv_cache_seq_addPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id, llama_pos,
              llama_pos, llama_pos)>>('llama_kv_cache_seq_add');
  late final _llama_kv_cache_seq_add = _llama_kv_cache_seq_addPtr.asFunction<
      void Function(ffi.Pointer<llama_context>, int, int, int, int)>();

  /// Integer division of the positions by factor of `d > 1`
  /// If the KV cache is RoPEd, the KV data is updated accordingly:
  /// - lazily on next llama_decode()
  /// - explicitly with llama_kv_cache_update()
  /// p0 < 0 : [0,  p1]
  /// p1 < 0 : [p0, inf)
  void llama_kv_cache_seq_div(
    ffi.Pointer<llama_context> ctx,
    int seq_id,
    int p0,
    int p1,
    int d,
  ) {
    return _llama_kv_cache_seq_div(
      ctx,
      seq_id,
      p0,
      p1,
      d,
    );
  }

  late final _llama_kv_cache_seq_divPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id, llama_pos,
              llama_pos, ffi.Int)>>('llama_kv_cache_seq_div');
  late final _llama_kv_cache_seq_div = _llama_kv_cache_seq_divPtr.asFunction<
      void Function(ffi.Pointer<llama_context>, int, int, int, int)>();

  /// Returns the largest position present in the KV cache for the specified sequence
  int llama_kv_cache_seq_pos_max(
    ffi.Pointer<llama_context> ctx,
    int seq_id,
  ) {
    return _llama_kv_cache_seq_pos_max(
      ctx,
      seq_id,
    );
  }

  late final _llama_kv_cache_seq_pos_maxPtr = _lookup<
      ffi.NativeFunction<
          llama_pos Function(ffi.Pointer<llama_context>,
              llama_seq_id)>>('llama_kv_cache_seq_pos_max');
  late final _llama_kv_cache_seq_pos_max = _llama_kv_cache_seq_pos_maxPtr
      .asFunction<int Function(ffi.Pointer<llama_context>, int)>();

  /// Defragment the KV cache
  /// This will be applied:
  /// - lazily on next llama_decode()
  /// - explicitly with llama_kv_cache_update()
  void llama_kv_cache_defrag(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_kv_cache_defrag(
      ctx,
    );
  }

  late final _llama_kv_cache_defragPtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_kv_cache_defrag');
  late final _llama_kv_cache_defrag = _llama_kv_cache_defragPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// Apply the KV cache updates (such as K-shifts, defragmentation, etc.)
  void llama_kv_cache_update(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_kv_cache_update(
      ctx,
    );
  }

  late final _llama_kv_cache_updatePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_kv_cache_update');
  late final _llama_kv_cache_update = _llama_kv_cache_updatePtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// Returns the maximum size in bytes of the state (rng, logits, embedding
  /// and kv_cache) - will often be smaller after compacting tokens
  int llama_get_state_size(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_get_state_size(
      ctx,
    );
  }

  late final _llama_get_state_sizePtr = _lookup<
          ffi.NativeFunction<ffi.Size Function(ffi.Pointer<llama_context>)>>(
      'llama_get_state_size');
  late final _llama_get_state_size = _llama_get_state_sizePtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  /// Copies the state to the specified destination address.
  /// Destination needs to have allocated enough memory.
  /// Returns the number of bytes copied
  int llama_copy_state_data(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> dst,
  ) {
    return _llama_copy_state_data(
      ctx,
      dst,
    );
  }

  late final _llama_copy_state_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<llama_context>,
              ffi.Pointer<ffi.Uint8>)>>('llama_copy_state_data');
  late final _llama_copy_state_data = _llama_copy_state_dataPtr.asFunction<
      int Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>)>();

  /// Set the state reading from the specified address
  /// Returns the number of bytes read
  int llama_set_state_data(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> src,
  ) {
    return _llama_set_state_data(
      ctx,
      src,
    );
  }

  late final _llama_set_state_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<llama_context>,
              ffi.Pointer<ffi.Uint8>)>>('llama_set_state_data');
  late final _llama_set_state_data = _llama_set_state_dataPtr.asFunction<
      int Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>)>();

  /// Save/load session file
  bool llama_load_session_file(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Char> path_session,
    ffi.Pointer<llama_token> tokens_out,
    int n_token_capacity,
    ffi.Pointer<ffi.Size> n_token_count_out,
  ) {
    return _llama_load_session_file(
      ctx,
      path_session,
      tokens_out,
      n_token_capacity,
      n_token_count_out,
    );
  }

  late final _llama_load_session_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<llama_token>,
              ffi.Size,
              ffi.Pointer<ffi.Size>)>>('llama_load_session_file');
  late final _llama_load_session_file = _llama_load_session_filePtr.asFunction<
      bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<llama_token>, int, ffi.Pointer<ffi.Size>)>();

  bool llama_save_session_file(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Char> path_session,
    ffi.Pointer<llama_token> tokens,
    int n_token_count,
  ) {
    return _llama_save_session_file(
      ctx,
      path_session,
      tokens,
      n_token_count,
    );
  }

  late final _llama_save_session_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
              ffi.Pointer<llama_token>, ffi.Size)>>('llama_save_session_file');
  late final _llama_save_session_file = _llama_save_session_filePtr.asFunction<
      bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<llama_token>, int)>();

  /// Return batch for single sequence of tokens starting at pos_0
  ///
  /// NOTE: this is a helper function to facilitate transition to the new batch API - avoid using it
  llama_batch llama_batch_get_one(
    ffi.Pointer<llama_token> tokens,
    int n_tokens,
    int pos_0,
    int seq_id,
  ) {
    return _llama_batch_get_one(
      tokens,
      n_tokens,
      pos_0,
      seq_id,
    );
  }

  late final _llama_batch_get_onePtr = _lookup<
      ffi.NativeFunction<
          llama_batch Function(ffi.Pointer<llama_token>, ffi.Int32, llama_pos,
              llama_seq_id)>>('llama_batch_get_one');
  late final _llama_batch_get_one = _llama_batch_get_onePtr.asFunction<
      llama_batch Function(ffi.Pointer<llama_token>, int, int, int)>();

  /// Allocates a batch of tokens on the heap that can hold a maximum of n_tokens
  /// Each token can be assigned up to n_seq_max sequence ids
  /// The batch has to be freed with llama_batch_free()
  /// If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)
  /// Otherwise, llama_batch.token will be allocated to store n_tokens llama_token
  /// The rest of the llama_batch members are allocated with size n_tokens
  /// All members are left uninitialized
  llama_batch llama_batch_init(
    int n_tokens,
    int embd,
    int n_seq_max,
  ) {
    return _llama_batch_init(
      n_tokens,
      embd,
      n_seq_max,
    );
  }

  late final _llama_batch_initPtr = _lookup<
      ffi.NativeFunction<
          llama_batch Function(
              ffi.Int32, ffi.Int32, ffi.Int32)>>('llama_batch_init');
  late final _llama_batch_init =
      _llama_batch_initPtr.asFunction<llama_batch Function(int, int, int)>();

  /// Frees a batch of tokens allocated with llama_batch_init()
  void llama_batch_free(
    llama_batch batch,
  ) {
    return _llama_batch_free(
      batch,
    );
  }

  late final _llama_batch_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(llama_batch)>>(
          'llama_batch_free');
  late final _llama_batch_free =
      _llama_batch_freePtr.asFunction<void Function(llama_batch)>();

  /// Positive return values does not mean a fatal error, but rather a warning.
  /// 0 - success
  /// 1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)
  /// < 0 - error
  int llama_decode(
    ffi.Pointer<llama_context> ctx,
    llama_batch batch,
  ) {
    return _llama_decode(
      ctx,
      batch,
    );
  }

  late final _llama_decodePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_context>, llama_batch)>>('llama_decode');
  late final _llama_decode = _llama_decodePtr
      .asFunction<int Function(ffi.Pointer<llama_context>, llama_batch)>();

  /// Set the number of threads used for decoding
  /// n_threads is the number of threads used for generation (single token)
  /// n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)
  void llama_set_n_threads(
    ffi.Pointer<llama_context> ctx,
    int n_threads,
    int n_threads_batch,
  ) {
    return _llama_set_n_threads(
      ctx,
      n_threads,
      n_threads_batch,
    );
  }

  late final _llama_set_n_threadsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, ffi.Uint32,
              ffi.Uint32)>>('llama_set_n_threads');
  late final _llama_set_n_threads = _llama_set_n_threadsPtr
      .asFunction<void Function(ffi.Pointer<llama_context>, int, int)>();

  /// Set abort callback
  void llama_set_abort_callback(
    ffi.Pointer<llama_context> ctx,
    ggml_abort_callback abort_callback,
    ffi.Pointer<ffi.Void> abort_callback_data,
  ) {
    return _llama_set_abort_callback(
      ctx,
      abort_callback,
      abort_callback_data,
    );
  }

  late final _llama_set_abort_callbackPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, ggml_abort_callback,
              ffi.Pointer<ffi.Void>)>>('llama_set_abort_callback');
  late final _llama_set_abort_callback =
      _llama_set_abort_callbackPtr.asFunction<
          void Function(ffi.Pointer<llama_context>, ggml_abort_callback,
              ffi.Pointer<ffi.Void>)>();

  /// Token logits obtained from the last call to llama_decode()
  /// The logits for the last token are stored in the last row
  /// Logits for which llama_batch.logits[i] == 0 are undefined
  /// Rows: n_tokens provided with llama_batch
  /// Cols: n_vocab
  ffi.Pointer<ffi.Float> llama_get_logits(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_get_logits(
      ctx,
    );
  }

  late final _llama_get_logitsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(
              ffi.Pointer<llama_context>)>>('llama_get_logits');
  late final _llama_get_logits = _llama_get_logitsPtr.asFunction<
      ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>)>();

  /// Logits for the ith token. Equivalent to:
  /// llama_get_logits(ctx) + i*n_vocab
  ffi.Pointer<ffi.Float> llama_get_logits_ith(
    ffi.Pointer<llama_context> ctx,
    int i,
  ) {
    return _llama_get_logits_ith(
      ctx,
      i,
    );
  }

  late final _llama_get_logits_ithPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(
              ffi.Pointer<llama_context>, ffi.Int32)>>('llama_get_logits_ith');
  late final _llama_get_logits_ith = _llama_get_logits_ithPtr.asFunction<
      ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, int)>();

  /// Get the embeddings for the input
  /// shape: [n_embd] (1-dimensional)
  ffi.Pointer<ffi.Float> llama_get_embeddings(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_get_embeddings(
      ctx,
    );
  }

  late final _llama_get_embeddingsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(
              ffi.Pointer<llama_context>)>>('llama_get_embeddings');
  late final _llama_get_embeddings = _llama_get_embeddingsPtr.asFunction<
      ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>)>();

  /// Get the embeddings for the ith sequence
  /// llama_get_embeddings(ctx) + i*n_embd
  ffi.Pointer<ffi.Float> llama_get_embeddings_ith(
    ffi.Pointer<llama_context> ctx,
    int i,
  ) {
    return _llama_get_embeddings_ith(
      ctx,
      i,
    );
  }

  late final _llama_get_embeddings_ithPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>,
              ffi.Int32)>>('llama_get_embeddings_ith');
  late final _llama_get_embeddings_ith =
      _llama_get_embeddings_ithPtr.asFunction<
          ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, int)>();

  /// Vocab
  ffi.Pointer<ffi.Char> llama_token_get_text(
    ffi.Pointer<llama_model> model,
    int token,
  ) {
    return _llama_token_get_text(
      model,
      token,
    );
  }

  late final _llama_token_get_textPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<llama_model>, llama_token)>>('llama_token_get_text');
  late final _llama_token_get_text = _llama_token_get_textPtr.asFunction<
      ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_model>, int)>();

  double llama_token_get_score(
    ffi.Pointer<llama_model> model,
    int token,
  ) {
    return _llama_token_get_score(
      model,
      token,
    );
  }

  late final _llama_token_get_scorePtr = _lookup<
      ffi.NativeFunction<
          ffi.Float Function(
              ffi.Pointer<llama_model>, llama_token)>>('llama_token_get_score');
  late final _llama_token_get_score = _llama_token_get_scorePtr
      .asFunction<double Function(ffi.Pointer<llama_model>, int)>();

  int llama_token_get_type(
    ffi.Pointer<llama_model> model,
    int token,
  ) {
    return _llama_token_get_type(
      model,
      token,
    );
  }

  late final _llama_token_get_typePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_model>, llama_token)>>('llama_token_get_type');
  late final _llama_token_get_type = _llama_token_get_typePtr
      .asFunction<int Function(ffi.Pointer<llama_model>, int)>();

  /// Special tokens
  int llama_token_bos(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_bos(
      model,
    );
  }

  late final _llama_token_bosPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_bos');
  late final _llama_token_bos =
      _llama_token_bosPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_eos(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_eos(
      model,
    );
  }

  late final _llama_token_eosPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_eos');
  late final _llama_token_eos =
      _llama_token_eosPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_nl(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_nl(
      model,
    );
  }

  late final _llama_token_nlPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_nl');
  late final _llama_token_nl =
      _llama_token_nlPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Returns -1 if unknown, 1 for true or 0 for false.
  int llama_add_bos_token(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_add_bos_token(
      model,
    );
  }

  late final _llama_add_bos_tokenPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_add_bos_token');
  late final _llama_add_bos_token = _llama_add_bos_tokenPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Returns -1 if unknown, 1 for true or 0 for false.
  int llama_add_eos_token(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_add_eos_token(
      model,
    );
  }

  late final _llama_add_eos_tokenPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_add_eos_token');
  late final _llama_add_eos_token = _llama_add_eos_tokenPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// codellama infill tokens
  int llama_token_prefix(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_prefix(
      model,
    );
  }

  late final _llama_token_prefixPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_prefix');
  late final _llama_token_prefix = _llama_token_prefixPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_middle(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_middle(
      model,
    );
  }

  late final _llama_token_middlePtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_middle');
  late final _llama_token_middle = _llama_token_middlePtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_suffix(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_suffix(
      model,
    );
  }

  late final _llama_token_suffixPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_suffix');
  late final _llama_token_suffix = _llama_token_suffixPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_eot(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_eot(
      model,
    );
  }

  late final _llama_token_eotPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_eot');
  late final _llama_token_eot =
      _llama_token_eotPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// @details Convert the provided text into tokens.
  /// @param tokens The tokens pointer must be large enough to hold the resulting tokens.
  /// @return Returns the number of tokens on success, no more than n_max_tokens
  /// @return Returns a negative number on failure - the number of tokens that would have been returned
  /// @param special Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext.
  /// Does not insert a leading space.
  int llama_tokenize(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> text,
    int text_len,
    ffi.Pointer<llama_token> tokens,
    int n_max_tokens,
    bool add_bos,
    bool special,
  ) {
    return _llama_tokenize(
      model,
      text,
      text_len,
      tokens,
      n_max_tokens,
      add_bos,
      special,
    );
  }

  late final _llama_tokenizePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_model>,
              ffi.Pointer<ffi.Char>,
              ffi.Int32,
              ffi.Pointer<llama_token>,
              ffi.Int32,
              ffi.Bool,
              ffi.Bool)>>('llama_tokenize');
  late final _llama_tokenize = _llama_tokenizePtr.asFunction<
      int Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>, int,
          ffi.Pointer<llama_token>, int, bool, bool)>();

  /// Token Id -> Piece.
  /// Uses the vocabulary in the provided context.
  /// Does not write null terminator to the buffer.
  /// User code is responsible to remove the leading whitespace of the first non-BOS token when decoding multiple tokens.
  int llama_token_to_piece(
    ffi.Pointer<llama_model> model,
    int token,
    ffi.Pointer<ffi.Char> buf,
    int length,
  ) {
    return _llama_token_to_piece(
      model,
      token,
      buf,
      length,
    );
  }

  late final _llama_token_to_piecePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<llama_model>, llama_token,
              ffi.Pointer<ffi.Char>, ffi.Int32)>>('llama_token_to_piece');
  late final _llama_token_to_piece = _llama_token_to_piecePtr.asFunction<
      int Function(
          ffi.Pointer<llama_model>, int, ffi.Pointer<ffi.Char>, int)>();

  /// Apply chat template. Inspired by hf apply_chat_template() on python.
  /// Both "model" and "custom_template" are optional, but at least one is required. "custom_template" has higher precedence than "model"
  /// NOTE: This function does not use a jinja parser. It only support a pre-defined list of template. See more: https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template
  /// @param tmpl A Jinja template to use for this chat. If this is nullptr, the modelâ€™s default chat template will be used instead.
  /// @param chat Pointer to a list of multiple llama_chat_message
  /// @param n_msg Number of llama_chat_message in this chat
  /// @param add_ass Whether to end the prompt with the token(s) that indicate the start of an assistant message.
  /// @param buf A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)
  /// @param length The size of the allocated buffer
  /// @return The total number of bytes of the formatted prompt. If is it larger than the size of buffer, you may need to re-alloc it and then re-apply the template.
  int llama_chat_apply_template(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> tmpl,
    ffi.Pointer<llama_chat_message> chat,
    int n_msg,
    bool add_ass,
    ffi.Pointer<ffi.Char> buf,
    int length,
  ) {
    return _llama_chat_apply_template(
      model,
      tmpl,
      chat,
      n_msg,
      add_ass,
      buf,
      length,
    );
  }

  late final _llama_chat_apply_templatePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_model>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<llama_chat_message>,
              ffi.Size,
              ffi.Bool,
              ffi.Pointer<ffi.Char>,
              ffi.Int32)>>('llama_chat_apply_template');
  late final _llama_chat_apply_template =
      _llama_chat_apply_templatePtr.asFunction<
          int Function(
              ffi.Pointer<llama_model>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<llama_chat_message>,
              int,
              bool,
              ffi.Pointer<ffi.Char>,
              int)>();

  /// Grammar
  ffi.Pointer<llama_grammar> llama_grammar_init(
    ffi.Pointer<ffi.Pointer<llama_grammar_element>> rules,
    int n_rules,
    int start_rule_index,
  ) {
    return _llama_grammar_init(
      rules,
      n_rules,
      start_rule_index,
    );
  }

  late final _llama_grammar_initPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_grammar> Function(
              ffi.Pointer<ffi.Pointer<llama_grammar_element>>,
              ffi.Size,
              ffi.Size)>>('llama_grammar_init');
  late final _llama_grammar_init = _llama_grammar_initPtr.asFunction<
      ffi.Pointer<llama_grammar> Function(
          ffi.Pointer<ffi.Pointer<llama_grammar_element>>, int, int)>();

  void llama_grammar_free(
    ffi.Pointer<llama_grammar> grammar,
  ) {
    return _llama_grammar_free(
      grammar,
    );
  }

  late final _llama_grammar_freePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_grammar>)>>(
      'llama_grammar_free');
  late final _llama_grammar_free = _llama_grammar_freePtr
      .asFunction<void Function(ffi.Pointer<llama_grammar>)>();

  ffi.Pointer<llama_grammar> llama_grammar_copy(
    ffi.Pointer<llama_grammar> grammar,
  ) {
    return _llama_grammar_copy(
      grammar,
    );
  }

  late final _llama_grammar_copyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_grammar> Function(
              ffi.Pointer<llama_grammar>)>>('llama_grammar_copy');
  late final _llama_grammar_copy = _llama_grammar_copyPtr.asFunction<
      ffi.Pointer<llama_grammar> Function(ffi.Pointer<llama_grammar>)>();

  /// Sets the current rng seed.
  void llama_set_rng_seed(
    ffi.Pointer<llama_context> ctx,
    int seed,
  ) {
    return _llama_set_rng_seed(
      ctx,
      seed,
    );
  }

  late final _llama_set_rng_seedPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>, ffi.Uint32)>>('llama_set_rng_seed');
  late final _llama_set_rng_seed = _llama_set_rng_seedPtr
      .asFunction<void Function(ffi.Pointer<llama_context>, int)>();

  /// @details Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix.
  /// @details Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.
  void llama_sample_repetition_penalties(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_token_data_array> candidates,
    ffi.Pointer<llama_token> last_tokens,
    int penalty_last_n,
    double penalty_repeat,
    double penalty_freq,
    double penalty_present,
  ) {
    return _llama_sample_repetition_penalties(
      ctx,
      candidates,
      last_tokens,
      penalty_last_n,
      penalty_repeat,
      penalty_freq,
      penalty_present,
    );
  }

  late final _llama_sample_repetition_penaltiesPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>,
              ffi.Pointer<llama_token>,
              ffi.Size,
              ffi.Float,
              ffi.Float,
              ffi.Float)>>('llama_sample_repetition_penalties');
  late final _llama_sample_repetition_penalties =
      _llama_sample_repetition_penaltiesPtr.asFunction<
          void Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>,
              ffi.Pointer<llama_token>,
              int,
              double,
              double,
              double)>();

  /// @details Apply classifier-free guidance to the logits as described in academic paper "Stay on topic with Classifier-Free Guidance" https://arxiv.org/abs/2306.17806
  /// @param logits Logits extracted from the original generation context.
  /// @param logits_guidance Logits extracted from a separate context from the same model. Other than a negative prompt at the beginning, it should have all generated and user input tokens copied from the main context.
  /// @param scale Guidance strength. 1.0f means no guidance. Higher values mean stronger guidance.
  void llama_sample_apply_guidance(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Float> logits,
    ffi.Pointer<ffi.Float> logits_guidance,
    double scale,
  ) {
    return _llama_sample_apply_guidance(
      ctx,
      logits,
      logits_guidance,
      scale,
    );
  }

  late final _llama_sample_apply_guidancePtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<ffi.Float>,
              ffi.Pointer<ffi.Float>,
              ffi.Float)>>('llama_sample_apply_guidance');
  late final _llama_sample_apply_guidance =
      _llama_sample_apply_guidancePtr.asFunction<
          void Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Float>,
              ffi.Pointer<ffi.Float>, double)>();

  /// @details Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.
  void llama_sample_softmax(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_token_data_array> candidates,
  ) {
    return _llama_sample_softmax(
      ctx,
      candidates,
    );
  }

  late final _llama_sample_softmaxPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>)>>('llama_sample_softmax');
  late final _llama_sample_softmax = _llama_sample_softmaxPtr.asFunction<
      void Function(
          ffi.Pointer<llama_context>, ffi.Pointer<llama_token_data_array>)>();

  /// @details Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
  void llama_sample_top_k(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_token_data_array> candidates,
    int k,
    int min_keep,
  ) {
    return _llama_sample_top_k(
      ctx,
      candidates,
      k,
      min_keep,
    );
  }

  late final _llama_sample_top_kPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>,
              ffi.Int32,
              ffi.Size)>>('llama_sample_top_k');
  late final _llama_sample_top_k = _llama_sample_top_kPtr.asFunction<
      void Function(ffi.Pointer<llama_context>,
          ffi.Pointer<llama_token_data_array>, int, int)>();

  /// @details Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
  void llama_sample_top_p(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_token_data_array> candidates,
    double p,
    int min_keep,
  ) {
    return _llama_sample_top_p(
      ctx,
      candidates,
      p,
      min_keep,
    );
  }

  late final _llama_sample_top_pPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>,
              ffi.Float,
              ffi.Size)>>('llama_sample_top_p');
  late final _llama_sample_top_p = _llama_sample_top_pPtr.asFunction<
      void Function(ffi.Pointer<llama_context>,
          ffi.Pointer<llama_token_data_array>, double, int)>();

  /// @details Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841
  void llama_sample_min_p(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_token_data_array> candidates,
    double p,
    int min_keep,
  ) {
    return _llama_sample_min_p(
      ctx,
      candidates,
      p,
      min_keep,
    );
  }

  late final _llama_sample_min_pPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>,
              ffi.Float,
              ffi.Size)>>('llama_sample_min_p');
  late final _llama_sample_min_p = _llama_sample_min_pPtr.asFunction<
      void Function(ffi.Pointer<llama_context>,
          ffi.Pointer<llama_token_data_array>, double, int)>();

  /// @details Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.
  void llama_sample_tail_free(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_token_data_array> candidates,
    double z,
    int min_keep,
  ) {
    return _llama_sample_tail_free(
      ctx,
      candidates,
      z,
      min_keep,
    );
  }

  late final _llama_sample_tail_freePtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>,
              ffi.Float,
              ffi.Size)>>('llama_sample_tail_free');
  late final _llama_sample_tail_free = _llama_sample_tail_freePtr.asFunction<
      void Function(ffi.Pointer<llama_context>,
          ffi.Pointer<llama_token_data_array>, double, int)>();

  /// @details Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.
  void llama_sample_typical(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_token_data_array> candidates,
    double p,
    int min_keep,
  ) {
    return _llama_sample_typical(
      ctx,
      candidates,
      p,
      min_keep,
    );
  }

  late final _llama_sample_typicalPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>,
              ffi.Float,
              ffi.Size)>>('llama_sample_typical');
  late final _llama_sample_typical = _llama_sample_typicalPtr.asFunction<
      void Function(ffi.Pointer<llama_context>,
          ffi.Pointer<llama_token_data_array>, double, int)>();

  /// @details Dynamic temperature implementation described in the paper https://arxiv.org/abs/2309.02772.
  void llama_sample_entropy(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_token_data_array> candidates_p,
    double min_temp,
    double max_temp,
    double exponent_val,
  ) {
    return _llama_sample_entropy(
      ctx,
      candidates_p,
      min_temp,
      max_temp,
      exponent_val,
    );
  }

  late final _llama_sample_entropyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>,
              ffi.Float,
              ffi.Float,
              ffi.Float)>>('llama_sample_entropy');
  late final _llama_sample_entropy = _llama_sample_entropyPtr.asFunction<
      void Function(ffi.Pointer<llama_context>,
          ffi.Pointer<llama_token_data_array>, double, double, double)>();

  void llama_sample_temp(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_token_data_array> candidates,
    double temp,
  ) {
    return _llama_sample_temp(
      ctx,
      candidates,
      temp,
    );
  }

  late final _llama_sample_tempPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>,
              ffi.Float)>>('llama_sample_temp');
  late final _llama_sample_temp = _llama_sample_tempPtr.asFunction<
      void Function(ffi.Pointer<llama_context>,
          ffi.Pointer<llama_token_data_array>, double)>();

  /// @details Apply constraints from grammar
  void llama_sample_grammar(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_token_data_array> candidates,
    ffi.Pointer<llama_grammar> grammar,
  ) {
    return _llama_sample_grammar(
      ctx,
      candidates,
      grammar,
    );
  }

  late final _llama_sample_grammarPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>,
              ffi.Pointer<llama_grammar>)>>('llama_sample_grammar');
  late final _llama_sample_grammar = _llama_sample_grammarPtr.asFunction<
      void Function(ffi.Pointer<llama_context>,
          ffi.Pointer<llama_token_data_array>, ffi.Pointer<llama_grammar>)>();

  /// @details Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
  /// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
  /// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
  /// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
  /// @param m The number of tokens considered in the estimation of `s_hat`. This is an arbitrary value that is used to calculate `s_hat`, which in turn helps to calculate the value of `k`. In the paper, they use `m = 100`, but you can experiment with different values to see how it affects the performance of the algorithm.
  /// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
  int llama_sample_token_mirostat(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_token_data_array> candidates,
    double tau,
    double eta,
    int m,
    ffi.Pointer<ffi.Float> mu,
  ) {
    return _llama_sample_token_mirostat(
      ctx,
      candidates,
      tau,
      eta,
      m,
      mu,
    );
  }

  late final _llama_sample_token_mirostatPtr = _lookup<
      ffi.NativeFunction<
          llama_token Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>,
              ffi.Float,
              ffi.Float,
              ffi.Int32,
              ffi.Pointer<ffi.Float>)>>('llama_sample_token_mirostat');
  late final _llama_sample_token_mirostat =
      _llama_sample_token_mirostatPtr.asFunction<
          int Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>,
              double,
              double,
              int,
              ffi.Pointer<ffi.Float>)>();

  /// @details Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
  /// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
  /// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
  /// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
  /// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
  int llama_sample_token_mirostat_v2(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_token_data_array> candidates,
    double tau,
    double eta,
    ffi.Pointer<ffi.Float> mu,
  ) {
    return _llama_sample_token_mirostat_v2(
      ctx,
      candidates,
      tau,
      eta,
      mu,
    );
  }

  late final _llama_sample_token_mirostat_v2Ptr = _lookup<
      ffi.NativeFunction<
          llama_token Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>,
              ffi.Float,
              ffi.Float,
              ffi.Pointer<ffi.Float>)>>('llama_sample_token_mirostat_v2');
  late final _llama_sample_token_mirostat_v2 =
      _llama_sample_token_mirostat_v2Ptr.asFunction<
          int Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>,
              double,
              double,
              ffi.Pointer<ffi.Float>)>();

  /// @details Selects the token with the highest probability.
  /// Does not compute the token probabilities. Use llama_sample_softmax() instead.
  int llama_sample_token_greedy(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_token_data_array> candidates,
  ) {
    return _llama_sample_token_greedy(
      ctx,
      candidates,
    );
  }

  late final _llama_sample_token_greedyPtr = _lookup<
          ffi.NativeFunction<
              llama_token Function(ffi.Pointer<llama_context>,
                  ffi.Pointer<llama_token_data_array>)>>(
      'llama_sample_token_greedy');
  late final _llama_sample_token_greedy =
      _llama_sample_token_greedyPtr.asFunction<
          int Function(ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>)>();

  /// @details Randomly selects a token from the candidates based on their probabilities.
  int llama_sample_token(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_token_data_array> candidates,
  ) {
    return _llama_sample_token(
      ctx,
      candidates,
    );
  }

  late final _llama_sample_tokenPtr = _lookup<
      ffi.NativeFunction<
          llama_token Function(ffi.Pointer<llama_context>,
              ffi.Pointer<llama_token_data_array>)>>('llama_sample_token');
  late final _llama_sample_token = _llama_sample_tokenPtr.asFunction<
      int Function(
          ffi.Pointer<llama_context>, ffi.Pointer<llama_token_data_array>)>();

  /// @details Accepts the sampled token into the grammar
  void llama_grammar_accept_token(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_grammar> grammar,
    int token,
  ) {
    return _llama_grammar_accept_token(
      ctx,
      grammar,
      token,
    );
  }

  late final _llama_grammar_accept_tokenPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_grammar>,
              llama_token)>>('llama_grammar_accept_token');
  late final _llama_grammar_accept_token =
      _llama_grammar_accept_tokenPtr.asFunction<
          void Function(
              ffi.Pointer<llama_context>, ffi.Pointer<llama_grammar>, int)>();

  /// @details Deterministically returns entire sentence constructed by a beam search.
  /// @param ctx Pointer to the llama_context.
  /// @param callback Invoked for each iteration of the beam_search loop, passing in beams_state.
  /// @param callback_data A pointer that is simply passed back to callback.
  /// @param n_beams Number of beams to use.
  /// @param n_past Number of tokens already evaluated.
  /// @param n_predict Maximum number of tokens to predict. EOS may occur earlier.
  void llama_beam_search(
    ffi.Pointer<llama_context> ctx,
    llama_beam_search_callback_fn_t callback,
    ffi.Pointer<ffi.Void> callback_data,
    int n_beams,
    int n_past,
    int n_predict,
  ) {
    return _llama_beam_search(
      ctx,
      callback,
      callback_data,
      n_beams,
      n_past,
      n_predict,
    );
  }

  late final _llama_beam_searchPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>,
              llama_beam_search_callback_fn_t,
              ffi.Pointer<ffi.Void>,
              ffi.Size,
              ffi.Int32,
              ffi.Int32)>>('llama_beam_search');
  late final _llama_beam_search = _llama_beam_searchPtr.asFunction<
      void Function(ffi.Pointer<llama_context>, llama_beam_search_callback_fn_t,
          ffi.Pointer<ffi.Void>, int, int, int)>();

  /// Performance information
  llama_timings llama_get_timings(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_get_timings(
      ctx,
    );
  }

  late final _llama_get_timingsPtr = _lookup<
          ffi
          .NativeFunction<llama_timings Function(ffi.Pointer<llama_context>)>>(
      'llama_get_timings');
  late final _llama_get_timings = _llama_get_timingsPtr
      .asFunction<llama_timings Function(ffi.Pointer<llama_context>)>();

  void llama_print_timings(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_print_timings(
      ctx,
    );
  }

  late final _llama_print_timingsPtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_print_timings');
  late final _llama_print_timings = _llama_print_timingsPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  void llama_reset_timings(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_reset_timings(
      ctx,
    );
  }

  late final _llama_reset_timingsPtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_reset_timings');
  late final _llama_reset_timings = _llama_reset_timingsPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// Print system information
  ffi.Pointer<ffi.Char> llama_print_system_info() {
    return _llama_print_system_info();
  }

  late final _llama_print_system_infoPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function()>>(
          'llama_print_system_info');
  late final _llama_print_system_info = _llama_print_system_infoPtr
      .asFunction<ffi.Pointer<ffi.Char> Function()>();

  /// Set callback for all future logging events.
  /// If this is not called, or NULL is supplied, everything is output on stderr.
  void llama_log_set(
    ggml_log_callback log_callback,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _llama_log_set(
      log_callback,
      user_data,
    );
  }

  late final _llama_log_setPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ggml_log_callback, ffi.Pointer<ffi.Void>)>>('llama_log_set');
  late final _llama_log_set = _llama_log_setPtr
      .asFunction<void Function(ggml_log_callback, ffi.Pointer<ffi.Void>)>();

  void llama_dump_timing_info_yaml(
    ffi.Pointer<FILE> stream,
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_dump_timing_info_yaml(
      stream,
      ctx,
    );
  }

  late final _llama_dump_timing_info_yamlPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<FILE>,
              ffi.Pointer<llama_context>)>>('llama_dump_timing_info_yaml');
  late final _llama_dump_timing_info_yaml =
      _llama_dump_timing_info_yamlPtr.asFunction<
          void Function(ffi.Pointer<FILE>, ffi.Pointer<llama_context>)>();

  /// @param config  Config for the recognizer.
  /// @return Return a pointer to the recognizer. The user has to invoke
  /// DestroyOnlineRecognizer() to free it to avoid memory leak.
  ffi.Pointer<SherpaOnnxOnlineRecognizer> CreateOnlineRecognizer(
    ffi.Pointer<SherpaOnnxOnlineRecognizerConfig> config,
  ) {
    return _CreateOnlineRecognizer(
      config,
    );
  }

  late final _CreateOnlineRecognizerPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<SherpaOnnxOnlineRecognizer> Function(
                  ffi.Pointer<SherpaOnnxOnlineRecognizerConfig>)>>(
      'CreateOnlineRecognizer');
  late final _CreateOnlineRecognizer = _CreateOnlineRecognizerPtr.asFunction<
      ffi.Pointer<SherpaOnnxOnlineRecognizer> Function(
          ffi.Pointer<SherpaOnnxOnlineRecognizerConfig>)>();

  /// Free a pointer returned by CreateOnlineRecognizer()
  ///
  /// @param p A pointer returned by CreateOnlineRecognizer()
  void DestroyOnlineRecognizer(
    ffi.Pointer<SherpaOnnxOnlineRecognizer> recognizer,
  ) {
    return _DestroyOnlineRecognizer(
      recognizer,
    );
  }

  late final _DestroyOnlineRecognizerPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<SherpaOnnxOnlineRecognizer>)>>(
      'DestroyOnlineRecognizer');
  late final _DestroyOnlineRecognizer = _DestroyOnlineRecognizerPtr.asFunction<
      void Function(ffi.Pointer<SherpaOnnxOnlineRecognizer>)>();

  /// Create an online stream for accepting wave samples.
  ///
  /// @param recognizer  A pointer returned by CreateOnlineRecognizer()
  /// @return Return a pointer to an OnlineStream. The user has to invoke
  /// DestroyOnlineStream() to free it to avoid memory leak.
  ffi.Pointer<SherpaOnnxOnlineStream> CreateOnlineStream(
    ffi.Pointer<SherpaOnnxOnlineRecognizer> recognizer,
  ) {
    return _CreateOnlineStream(
      recognizer,
    );
  }

  late final _CreateOnlineStreamPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<SherpaOnnxOnlineStream> Function(
              ffi.Pointer<SherpaOnnxOnlineRecognizer>)>>('CreateOnlineStream');
  late final _CreateOnlineStream = _CreateOnlineStreamPtr.asFunction<
      ffi.Pointer<SherpaOnnxOnlineStream> Function(
          ffi.Pointer<SherpaOnnxOnlineRecognizer>)>();

  /// Create an online stream for accepting wave samples with the specified hot
  /// words.
  ///
  /// @param recognizer  A pointer returned by CreateOnlineRecognizer()
  /// @return Return a pointer to an OnlineStream. The user has to invoke
  /// DestroyOnlineStream() to free it to avoid memory leak.
  ffi.Pointer<SherpaOnnxOnlineStream> CreateOnlineStreamWithHotwords(
    ffi.Pointer<SherpaOnnxOnlineRecognizer> recognizer,
    ffi.Pointer<ffi.Char> hotwords,
  ) {
    return _CreateOnlineStreamWithHotwords(
      recognizer,
      hotwords,
    );
  }

  late final _CreateOnlineStreamWithHotwordsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<SherpaOnnxOnlineStream> Function(
              ffi.Pointer<SherpaOnnxOnlineRecognizer>,
              ffi.Pointer<ffi.Char>)>>('CreateOnlineStreamWithHotwords');
  late final _CreateOnlineStreamWithHotwords =
      _CreateOnlineStreamWithHotwordsPtr.asFunction<
          ffi.Pointer<SherpaOnnxOnlineStream> Function(
              ffi.Pointer<SherpaOnnxOnlineRecognizer>,
              ffi.Pointer<ffi.Char>)>();

  /// Destroy an online stream.
  ///
  /// @param stream A pointer returned by CreateOnlineStream()
  void DestroyOnlineStream(
    ffi.Pointer<SherpaOnnxOnlineStream> stream,
  ) {
    return _DestroyOnlineStream(
      stream,
    );
  }

  late final _DestroyOnlineStreamPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<SherpaOnnxOnlineStream>)>>('DestroyOnlineStream');
  late final _DestroyOnlineStream = _DestroyOnlineStreamPtr.asFunction<
      void Function(ffi.Pointer<SherpaOnnxOnlineStream>)>();

  /// Accept input audio samples and compute the features.
  /// The user has to invoke DecodeOnlineStream() to run the neural network and
  /// decoding.
  ///
  /// @param stream  A pointer returned by CreateOnlineStream().
  /// @param sample_rate  Sample rate of the input samples. If it is different
  /// from config.feat_config.sample_rate, we will do
  /// resampling inside sherpa-onnx.
  /// @param samples A pointer to a 1-D array containing audio samples.
  /// The range of samples has to be normalized to [-1, 1].
  /// @param n  Number of elements in the samples array.
  void AcceptWaveform(
    ffi.Pointer<SherpaOnnxOnlineStream> stream,
    int sample_rate,
    ffi.Pointer<ffi.Float> samples,
    int n,
  ) {
    return _AcceptWaveform(
      stream,
      sample_rate,
      samples,
      n,
    );
  }

  late final _AcceptWaveformPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<SherpaOnnxOnlineStream>, ffi.Int32,
              ffi.Pointer<ffi.Float>, ffi.Int32)>>('AcceptWaveform');
  late final _AcceptWaveform = _AcceptWaveformPtr.asFunction<
      void Function(ffi.Pointer<SherpaOnnxOnlineStream>, int,
          ffi.Pointer<ffi.Float>, int)>();

  /// Return 1 if there are enough number of feature frames for decoding.
  /// Return 0 otherwise.
  ///
  /// @param recognizer  A pointer returned by CreateOnlineRecognizer
  /// @param stream  A pointer returned by CreateOnlineStream
  int IsOnlineStreamReady(
    ffi.Pointer<SherpaOnnxOnlineRecognizer> recognizer,
    ffi.Pointer<SherpaOnnxOnlineStream> stream,
  ) {
    return _IsOnlineStreamReady(
      recognizer,
      stream,
    );
  }

  late final _IsOnlineStreamReadyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<SherpaOnnxOnlineRecognizer>,
              ffi.Pointer<SherpaOnnxOnlineStream>)>>('IsOnlineStreamReady');
  late final _IsOnlineStreamReady = _IsOnlineStreamReadyPtr.asFunction<
      int Function(ffi.Pointer<SherpaOnnxOnlineRecognizer>,
          ffi.Pointer<SherpaOnnxOnlineStream>)>();

  /// Call this function to run the neural network model and decoding.
  ///
  /// Precondition for this function: IsOnlineStreamReady() MUST return 1.
  ///
  /// Usage example:
  ///
  /// while (IsOnlineStreamReady(recognizer, stream)) {
  /// DecodeOnlineStream(recognizer, stream);
  /// }
  void DecodeOnlineStream(
    ffi.Pointer<SherpaOnnxOnlineRecognizer> recognizer,
    ffi.Pointer<SherpaOnnxOnlineStream> stream,
  ) {
    return _DecodeOnlineStream(
      recognizer,
      stream,
    );
  }

  late final _DecodeOnlineStreamPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<SherpaOnnxOnlineRecognizer>,
              ffi.Pointer<SherpaOnnxOnlineStream>)>>('DecodeOnlineStream');
  late final _DecodeOnlineStream = _DecodeOnlineStreamPtr.asFunction<
      void Function(ffi.Pointer<SherpaOnnxOnlineRecognizer>,
          ffi.Pointer<SherpaOnnxOnlineStream>)>();

  /// This function is similar to DecodeOnlineStream(). It decodes multiple
  /// OnlineStream in parallel.
  ///
  /// Caution: The caller has to ensure each OnlineStream is ready, i.e.,
  /// IsOnlineStreamReady() for that stream should return 1.
  ///
  /// @param recognizer  A pointer returned by CreateOnlineRecognizer()
  /// @param streams  A pointer array containing pointers returned by
  /// CreateOnlineRecognizer()
  /// @param n  Number of elements in the given streams array.
  void DecodeMultipleOnlineStreams(
    ffi.Pointer<SherpaOnnxOnlineRecognizer> recognizer,
    ffi.Pointer<ffi.Pointer<SherpaOnnxOnlineStream>> streams,
    int n,
  ) {
    return _DecodeMultipleOnlineStreams(
      recognizer,
      streams,
      n,
    );
  }

  late final _DecodeMultipleOnlineStreamsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<SherpaOnnxOnlineRecognizer>,
              ffi.Pointer<ffi.Pointer<SherpaOnnxOnlineStream>>,
              ffi.Int32)>>('DecodeMultipleOnlineStreams');
  late final _DecodeMultipleOnlineStreams =
      _DecodeMultipleOnlineStreamsPtr.asFunction<
          void Function(ffi.Pointer<SherpaOnnxOnlineRecognizer>,
              ffi.Pointer<ffi.Pointer<SherpaOnnxOnlineStream>>, int)>();

  /// Get the decoding results so far for an OnlineStream.
  ///
  /// @param recognizer A pointer returned by CreateOnlineRecognizer().
  /// @param stream A pointer returned by CreateOnlineStream().
  /// @return A pointer containing the result. The user has to invoke
  /// DestroyOnlineRecognizerResult() to free the returned pointer to
  /// avoid memory leak.
  ffi.Pointer<SherpaOnnxOnlineRecognizerResult> GetOnlineStreamResult(
    ffi.Pointer<SherpaOnnxOnlineRecognizer> recognizer,
    ffi.Pointer<SherpaOnnxOnlineStream> stream,
  ) {
    return _GetOnlineStreamResult(
      recognizer,
      stream,
    );
  }

  late final _GetOnlineStreamResultPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<SherpaOnnxOnlineRecognizerResult> Function(
              ffi.Pointer<SherpaOnnxOnlineRecognizer>,
              ffi.Pointer<SherpaOnnxOnlineStream>)>>('GetOnlineStreamResult');
  late final _GetOnlineStreamResult = _GetOnlineStreamResultPtr.asFunction<
      ffi.Pointer<SherpaOnnxOnlineRecognizerResult> Function(
          ffi.Pointer<SherpaOnnxOnlineRecognizer>,
          ffi.Pointer<SherpaOnnxOnlineStream>)>();

  /// Destroy the pointer returned by GetOnlineStreamResult().
  ///
  /// @param r A pointer returned by GetOnlineStreamResult()
  void DestroyOnlineRecognizerResult(
    ffi.Pointer<SherpaOnnxOnlineRecognizerResult> r,
  ) {
    return _DestroyOnlineRecognizerResult(
      r,
    );
  }

  late final _DestroyOnlineRecognizerResultPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<SherpaOnnxOnlineRecognizerResult>)>>(
      'DestroyOnlineRecognizerResult');
  late final _DestroyOnlineRecognizerResult =
      _DestroyOnlineRecognizerResultPtr.asFunction<
          void Function(ffi.Pointer<SherpaOnnxOnlineRecognizerResult>)>();

  /// Reset an OnlineStream , which clears the neural network model state
  /// and the state for decoding.
  ///
  /// @param recognizer A pointer returned by CreateOnlineRecognizer().
  /// @param stream A pointer returned by CreateOnlineStream
  void Reset(
    ffi.Pointer<SherpaOnnxOnlineRecognizer> recognizer,
    ffi.Pointer<SherpaOnnxOnlineStream> stream,
  ) {
    return _Reset(
      recognizer,
      stream,
    );
  }

  late final _ResetPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<SherpaOnnxOnlineRecognizer>,
              ffi.Pointer<SherpaOnnxOnlineStream>)>>('Reset');
  late final _Reset = _ResetPtr.asFunction<
      void Function(ffi.Pointer<SherpaOnnxOnlineRecognizer>,
          ffi.Pointer<SherpaOnnxOnlineStream>)>();

  /// Signal that no more audio samples would be available.
  /// After this call, you cannot call AcceptWaveform() any more.
  ///
  /// @param stream A pointer returned by CreateOnlineStream()
  void InputFinished(
    ffi.Pointer<SherpaOnnxOnlineStream> stream,
  ) {
    return _InputFinished(
      stream,
    );
  }

  late final _InputFinishedPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<SherpaOnnxOnlineStream>)>>('InputFinished');
  late final _InputFinished = _InputFinishedPtr.asFunction<
      void Function(ffi.Pointer<SherpaOnnxOnlineStream>)>();

  /// Return 1 if an endpoint has been detected.
  ///
  /// @param recognizer A pointer returned by CreateOnlineRecognizer()
  /// @param stream A pointer returned by CreateOnlineStream()
  /// @return Return 1 if an endpoint is detected. Return 0 otherwise.
  int IsEndpoint(
    ffi.Pointer<SherpaOnnxOnlineRecognizer> recognizer,
    ffi.Pointer<SherpaOnnxOnlineStream> stream,
  ) {
    return _IsEndpoint(
      recognizer,
      stream,
    );
  }

  late final _IsEndpointPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<SherpaOnnxOnlineRecognizer>,
              ffi.Pointer<SherpaOnnxOnlineStream>)>>('IsEndpoint');
  late final _IsEndpoint = _IsEndpointPtr.asFunction<
      int Function(ffi.Pointer<SherpaOnnxOnlineRecognizer>,
          ffi.Pointer<SherpaOnnxOnlineStream>)>();

  /// Create a display object. Must be freed using DestroyDisplay to avoid
  /// memory leak.
  ffi.Pointer<SherpaOnnxDisplay> CreateDisplay(
    int max_word_per_line,
  ) {
    return _CreateDisplay(
      max_word_per_line,
    );
  }

  late final _CreateDisplayPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<SherpaOnnxDisplay> Function(ffi.Int32)>>('CreateDisplay');
  late final _CreateDisplay = _CreateDisplayPtr.asFunction<
      ffi.Pointer<SherpaOnnxDisplay> Function(int)>();

  void DestroyDisplay(
    ffi.Pointer<SherpaOnnxDisplay> display,
  ) {
    return _DestroyDisplay(
      display,
    );
  }

  late final _DestroyDisplayPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<SherpaOnnxDisplay>)>>('DestroyDisplay');
  late final _DestroyDisplay = _DestroyDisplayPtr.asFunction<
      void Function(ffi.Pointer<SherpaOnnxDisplay>)>();

  /// Print the result.
  void SherpaOnnxPrint(
    ffi.Pointer<SherpaOnnxDisplay> display,
    int idx,
    ffi.Pointer<ffi.Char> s,
  ) {
    return _SherpaOnnxPrint(
      display,
      idx,
      s,
    );
  }

  late final _SherpaOnnxPrintPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<SherpaOnnxDisplay>, ffi.Int32,
              ffi.Pointer<ffi.Char>)>>('SherpaOnnxPrint');
  late final _SherpaOnnxPrint = _SherpaOnnxPrintPtr.asFunction<
      void Function(
          ffi.Pointer<SherpaOnnxDisplay>, int, ffi.Pointer<ffi.Char>)>();

  /// @param config  Config for the recognizer.
  /// @return Return a pointer to the recognizer. The user has to invoke
  /// DestroyOfflineRecognizer() to free it to avoid memory leak.
  ffi.Pointer<SherpaOnnxOfflineRecognizer> CreateOfflineRecognizer(
    ffi.Pointer<SherpaOnnxOfflineRecognizerConfig> config,
  ) {
    return _CreateOfflineRecognizer(
      config,
    );
  }

  late final _CreateOfflineRecognizerPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<SherpaOnnxOfflineRecognizer> Function(
                  ffi.Pointer<SherpaOnnxOfflineRecognizerConfig>)>>(
      'CreateOfflineRecognizer');
  late final _CreateOfflineRecognizer = _CreateOfflineRecognizerPtr.asFunction<
      ffi.Pointer<SherpaOnnxOfflineRecognizer> Function(
          ffi.Pointer<SherpaOnnxOfflineRecognizerConfig>)>();

  /// Free a pointer returned by CreateOfflineRecognizer()
  ///
  /// @param p A pointer returned by CreateOfflineRecognizer()
  void DestroyOfflineRecognizer(
    ffi.Pointer<SherpaOnnxOfflineRecognizer> recognizer,
  ) {
    return _DestroyOfflineRecognizer(
      recognizer,
    );
  }

  late final _DestroyOfflineRecognizerPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<SherpaOnnxOfflineRecognizer>)>>(
      'DestroyOfflineRecognizer');
  late final _DestroyOfflineRecognizer = _DestroyOfflineRecognizerPtr
      .asFunction<void Function(ffi.Pointer<SherpaOnnxOfflineRecognizer>)>();

  /// Create an offline stream for accepting wave samples.
  ///
  /// @param recognizer  A pointer returned by CreateOfflineRecognizer()
  /// @return Return a pointer to an OfflineStream. The user has to invoke
  /// DestroyOfflineStream() to free it to avoid memory leak.
  ffi.Pointer<SherpaOnnxOfflineStream> CreateOfflineStream(
    ffi.Pointer<SherpaOnnxOfflineRecognizer> recognizer,
  ) {
    return _CreateOfflineStream(
      recognizer,
    );
  }

  late final _CreateOfflineStreamPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<SherpaOnnxOfflineStream> Function(
                  ffi.Pointer<SherpaOnnxOfflineRecognizer>)>>(
      'CreateOfflineStream');
  late final _CreateOfflineStream = _CreateOfflineStreamPtr.asFunction<
      ffi.Pointer<SherpaOnnxOfflineStream> Function(
          ffi.Pointer<SherpaOnnxOfflineRecognizer>)>();

  /// Destroy an offline stream.
  ///
  /// @param stream A pointer returned by CreateOfflineStream()
  void DestroyOfflineStream(
    ffi.Pointer<SherpaOnnxOfflineStream> stream,
  ) {
    return _DestroyOfflineStream(
      stream,
    );
  }

  late final _DestroyOfflineStreamPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<SherpaOnnxOfflineStream>)>>('DestroyOfflineStream');
  late final _DestroyOfflineStream = _DestroyOfflineStreamPtr.asFunction<
      void Function(ffi.Pointer<SherpaOnnxOfflineStream>)>();

  /// Accept input audio samples and compute the features.
  /// The user has to invoke DecodeOfflineStream() to run the neural network and
  /// decoding.
  ///
  /// @param stream  A pointer returned by CreateOfflineStream().
  /// @param sample_rate  Sample rate of the input samples. If it is different
  /// from config.feat_config.sample_rate, we will do
  /// resampling inside sherpa-onnx.
  /// @param samples A pointer to a 1-D array containing audio samples.
  /// The range of samples has to be normalized to [-1, 1].
  /// @param n  Number of elements in the samples array.
  ///
  /// @caution: For each offline stream, please invoke this function only once!
  void AcceptWaveformOffline(
    ffi.Pointer<SherpaOnnxOfflineStream> stream,
    int sample_rate,
    ffi.Pointer<ffi.Float> samples,
    int n,
  ) {
    return _AcceptWaveformOffline(
      stream,
      sample_rate,
      samples,
      n,
    );
  }

  late final _AcceptWaveformOfflinePtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<SherpaOnnxOfflineStream>, ffi.Int32,
              ffi.Pointer<ffi.Float>, ffi.Int32)>>('AcceptWaveformOffline');
  late final _AcceptWaveformOffline = _AcceptWaveformOfflinePtr.asFunction<
      void Function(ffi.Pointer<SherpaOnnxOfflineStream>, int,
          ffi.Pointer<ffi.Float>, int)>();

  /// Decode an offline stream.
  ///
  /// We assume you have invoked AcceptWaveformOffline() for the given stream
  /// before calling this function.
  ///
  /// @param recognizer A pointer returned by CreateOfflineRecognizer().
  /// @param stream A pointer returned by CreateOfflineStream()
  void DecodeOfflineStream(
    ffi.Pointer<SherpaOnnxOfflineRecognizer> recognizer,
    ffi.Pointer<SherpaOnnxOfflineStream> stream,
  ) {
    return _DecodeOfflineStream(
      recognizer,
      stream,
    );
  }

  late final _DecodeOfflineStreamPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<SherpaOnnxOfflineRecognizer>,
              ffi.Pointer<SherpaOnnxOfflineStream>)>>('DecodeOfflineStream');
  late final _DecodeOfflineStream = _DecodeOfflineStreamPtr.asFunction<
      void Function(ffi.Pointer<SherpaOnnxOfflineRecognizer>,
          ffi.Pointer<SherpaOnnxOfflineStream>)>();

  /// Decode a list offline streams in parallel.
  ///
  /// We assume you have invoked AcceptWaveformOffline() for each stream
  /// before calling this function.
  ///
  /// @param recognizer A pointer returned by CreateOfflineRecognizer().
  /// @param streams A pointer pointer array containing pointers returned
  /// by CreateOfflineStream().
  /// @param n Number of entries in the given streams.
  void DecodeMultipleOfflineStreams(
    ffi.Pointer<SherpaOnnxOfflineRecognizer> recognizer,
    ffi.Pointer<ffi.Pointer<SherpaOnnxOfflineStream>> streams,
    int n,
  ) {
    return _DecodeMultipleOfflineStreams(
      recognizer,
      streams,
      n,
    );
  }

  late final _DecodeMultipleOfflineStreamsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<SherpaOnnxOfflineRecognizer>,
              ffi.Pointer<ffi.Pointer<SherpaOnnxOfflineStream>>,
              ffi.Int32)>>('DecodeMultipleOfflineStreams');
  late final _DecodeMultipleOfflineStreams =
      _DecodeMultipleOfflineStreamsPtr.asFunction<
          void Function(ffi.Pointer<SherpaOnnxOfflineRecognizer>,
              ffi.Pointer<ffi.Pointer<SherpaOnnxOfflineStream>>, int)>();

  /// Get the result of the offline stream.
  ///
  /// We assume you have called DecodeOfflineStream() or
  /// DecodeMultipleOfflineStreams() with the given stream before calling
  /// this function.
  ///
  /// @param stream A pointer returned by CreateOfflineStream().
  /// @return Return a pointer to the result. The user has to invoke
  /// DestroyOnlineRecognizerResult() to free the returned pointer to
  /// avoid memory leak.
  ffi.Pointer<SherpaOnnxOfflineRecognizerResult> GetOfflineStreamResult(
    ffi.Pointer<SherpaOnnxOfflineStream> stream,
  ) {
    return _GetOfflineStreamResult(
      stream,
    );
  }

  late final _GetOfflineStreamResultPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<SherpaOnnxOfflineRecognizerResult> Function(
              ffi.Pointer<SherpaOnnxOfflineStream>)>>('GetOfflineStreamResult');
  late final _GetOfflineStreamResult = _GetOfflineStreamResultPtr.asFunction<
      ffi.Pointer<SherpaOnnxOfflineRecognizerResult> Function(
          ffi.Pointer<SherpaOnnxOfflineStream>)>();

  /// Destroy the pointer returned by GetOfflineStreamResult().
  ///
  /// @param r A pointer returned by GetOfflineStreamResult()
  void DestroyOfflineRecognizerResult(
    ffi.Pointer<SherpaOnnxOfflineRecognizerResult> r,
  ) {
    return _DestroyOfflineRecognizerResult(
      r,
    );
  }

  late final _DestroyOfflineRecognizerResultPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ffi.Pointer<SherpaOnnxOfflineRecognizerResult>)>>(
      'DestroyOfflineRecognizerResult');
  late final _DestroyOfflineRecognizerResult =
      _DestroyOfflineRecognizerResultPtr.asFunction<
          void Function(ffi.Pointer<SherpaOnnxOfflineRecognizerResult>)>();

  /// Return an instance of circular buffer. The user has to use
  /// SherpaOnnxDestroyCircularBuffer() to free the returned pointer to avoid
  /// memory leak.
  ffi.Pointer<SherpaOnnxCircularBuffer> SherpaOnnxCreateCircularBuffer(
    int capacity,
  ) {
    return _SherpaOnnxCreateCircularBuffer(
      capacity,
    );
  }

  late final _SherpaOnnxCreateCircularBufferPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<SherpaOnnxCircularBuffer> Function(
              ffi.Int32)>>('SherpaOnnxCreateCircularBuffer');
  late final _SherpaOnnxCreateCircularBuffer =
      _SherpaOnnxCreateCircularBufferPtr.asFunction<
          ffi.Pointer<SherpaOnnxCircularBuffer> Function(int)>();

  /// Free the pointer returned by SherpaOnnxCreateCircularBuffer()
  void SherpaOnnxDestroyCircularBuffer(
    ffi.Pointer<SherpaOnnxCircularBuffer> buffer,
  ) {
    return _SherpaOnnxDestroyCircularBuffer(
      buffer,
    );
  }

  late final _SherpaOnnxDestroyCircularBufferPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<SherpaOnnxCircularBuffer>)>>(
      'SherpaOnnxDestroyCircularBuffer');
  late final _SherpaOnnxDestroyCircularBuffer =
      _SherpaOnnxDestroyCircularBufferPtr.asFunction<
          void Function(ffi.Pointer<SherpaOnnxCircularBuffer>)>();

  void SherpaOnnxCircularBufferPush(
    ffi.Pointer<SherpaOnnxCircularBuffer> buffer,
    ffi.Pointer<ffi.Float> p,
    int n,
  ) {
    return _SherpaOnnxCircularBufferPush(
      buffer,
      p,
      n,
    );
  }

  late final _SherpaOnnxCircularBufferPushPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<SherpaOnnxCircularBuffer>,
              ffi.Pointer<ffi.Float>,
              ffi.Int32)>>('SherpaOnnxCircularBufferPush');
  late final _SherpaOnnxCircularBufferPush =
      _SherpaOnnxCircularBufferPushPtr.asFunction<
          void Function(ffi.Pointer<SherpaOnnxCircularBuffer>,
              ffi.Pointer<ffi.Float>, int)>();

  /// Return n samples starting at the given index.
  ///
  /// Return a pointer to an array containing n samples starting at start_index.
  /// The user has to use SherpaOnnxCircularBufferFree() to free the returned
  /// pointer to avoid memory leak.
  ffi.Pointer<ffi.Float> SherpaOnnxCircularBufferGet(
    ffi.Pointer<SherpaOnnxCircularBuffer> buffer,
    int start_index,
    int n,
  ) {
    return _SherpaOnnxCircularBufferGet(
      buffer,
      start_index,
      n,
    );
  }

  late final _SherpaOnnxCircularBufferGetPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(ffi.Pointer<SherpaOnnxCircularBuffer>,
              ffi.Int32, ffi.Int32)>>('SherpaOnnxCircularBufferGet');
  late final _SherpaOnnxCircularBufferGet =
      _SherpaOnnxCircularBufferGetPtr.asFunction<
          ffi.Pointer<ffi.Float> Function(
              ffi.Pointer<SherpaOnnxCircularBuffer>, int, int)>();

  /// Free the pointer returned by SherpaOnnxCircularBufferGet().
  void SherpaOnnxCircularBufferFree(
    ffi.Pointer<ffi.Float> p,
  ) {
    return _SherpaOnnxCircularBufferFree(
      p,
    );
  }

  late final _SherpaOnnxCircularBufferFreePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ffi.Float>)>>(
          'SherpaOnnxCircularBufferFree');
  late final _SherpaOnnxCircularBufferFree = _SherpaOnnxCircularBufferFreePtr
      .asFunction<void Function(ffi.Pointer<ffi.Float>)>();

  /// Remove n elements from the buffer
  void SherpaOnnxCircularBufferPop(
    ffi.Pointer<SherpaOnnxCircularBuffer> buffer,
    int n,
  ) {
    return _SherpaOnnxCircularBufferPop(
      buffer,
      n,
    );
  }

  late final _SherpaOnnxCircularBufferPopPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<SherpaOnnxCircularBuffer>,
              ffi.Int32)>>('SherpaOnnxCircularBufferPop');
  late final _SherpaOnnxCircularBufferPop = _SherpaOnnxCircularBufferPopPtr
      .asFunction<void Function(ffi.Pointer<SherpaOnnxCircularBuffer>, int)>();

  /// Return number of elements in the buffer.
  int SherpaOnnxCircularBufferSize(
    ffi.Pointer<SherpaOnnxCircularBuffer> buffer,
  ) {
    return _SherpaOnnxCircularBufferSize(
      buffer,
    );
  }

  late final _SherpaOnnxCircularBufferSizePtr = _lookup<
          ffi.NativeFunction<
              ffi.Int32 Function(ffi.Pointer<SherpaOnnxCircularBuffer>)>>(
      'SherpaOnnxCircularBufferSize');
  late final _SherpaOnnxCircularBufferSize = _SherpaOnnxCircularBufferSizePtr
      .asFunction<int Function(ffi.Pointer<SherpaOnnxCircularBuffer>)>();

  /// Return the head of the buffer. It's always non-decreasing until you
  /// invoke SherpaOnnxCircularBufferReset() which resets head to 0.
  int SherpaOnnxCircularBufferHead(
    ffi.Pointer<SherpaOnnxCircularBuffer> buffer,
  ) {
    return _SherpaOnnxCircularBufferHead(
      buffer,
    );
  }

  late final _SherpaOnnxCircularBufferHeadPtr = _lookup<
          ffi.NativeFunction<
              ffi.Int32 Function(ffi.Pointer<SherpaOnnxCircularBuffer>)>>(
      'SherpaOnnxCircularBufferHead');
  late final _SherpaOnnxCircularBufferHead = _SherpaOnnxCircularBufferHeadPtr
      .asFunction<int Function(ffi.Pointer<SherpaOnnxCircularBuffer>)>();

  /// Clear all elements in the buffer
  void SherpaOnnxCircularBufferReset(
    ffi.Pointer<SherpaOnnxCircularBuffer> buffer,
  ) {
    return _SherpaOnnxCircularBufferReset(
      buffer,
    );
  }

  late final _SherpaOnnxCircularBufferResetPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<SherpaOnnxCircularBuffer>)>>(
      'SherpaOnnxCircularBufferReset');
  late final _SherpaOnnxCircularBufferReset = _SherpaOnnxCircularBufferResetPtr
      .asFunction<void Function(ffi.Pointer<SherpaOnnxCircularBuffer>)>();

  /// Return an instance of VoiceActivityDetector.
  /// The user has to use SherpaOnnxDestroyVoiceActivityDetector() to free
  /// the returned pointer to avoid memory leak.
  ffi.Pointer<SherpaOnnxVoiceActivityDetector>
      SherpaOnnxCreateVoiceActivityDetector(
    ffi.Pointer<SherpaOnnxVadModelConfig> config,
    double buffer_size_in_seconds,
  ) {
    return _SherpaOnnxCreateVoiceActivityDetector(
      config,
      buffer_size_in_seconds,
    );
  }

  late final _SherpaOnnxCreateVoiceActivityDetectorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<SherpaOnnxVoiceActivityDetector> Function(
              ffi.Pointer<SherpaOnnxVadModelConfig>,
              ffi.Float)>>('SherpaOnnxCreateVoiceActivityDetector');
  late final _SherpaOnnxCreateVoiceActivityDetector =
      _SherpaOnnxCreateVoiceActivityDetectorPtr.asFunction<
          ffi.Pointer<SherpaOnnxVoiceActivityDetector> Function(
              ffi.Pointer<SherpaOnnxVadModelConfig>, double)>();

  void SherpaOnnxDestroyVoiceActivityDetector(
    ffi.Pointer<SherpaOnnxVoiceActivityDetector> p,
  ) {
    return _SherpaOnnxDestroyVoiceActivityDetector(
      p,
    );
  }

  late final _SherpaOnnxDestroyVoiceActivityDetectorPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<SherpaOnnxVoiceActivityDetector>)>>(
      'SherpaOnnxDestroyVoiceActivityDetector');
  late final _SherpaOnnxDestroyVoiceActivityDetector =
      _SherpaOnnxDestroyVoiceActivityDetectorPtr.asFunction<
          void Function(ffi.Pointer<SherpaOnnxVoiceActivityDetector>)>();

  void SherpaOnnxVoiceActivityDetectorAcceptWaveform(
    ffi.Pointer<SherpaOnnxVoiceActivityDetector> p,
    ffi.Pointer<ffi.Float> samples,
    int n,
  ) {
    return _SherpaOnnxVoiceActivityDetectorAcceptWaveform(
      p,
      samples,
      n,
    );
  }

  late final _SherpaOnnxVoiceActivityDetectorAcceptWaveformPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<SherpaOnnxVoiceActivityDetector>,
              ffi.Pointer<ffi.Float>,
              ffi.Int32)>>('SherpaOnnxVoiceActivityDetectorAcceptWaveform');
  late final _SherpaOnnxVoiceActivityDetectorAcceptWaveform =
      _SherpaOnnxVoiceActivityDetectorAcceptWaveformPtr.asFunction<
          void Function(ffi.Pointer<SherpaOnnxVoiceActivityDetector>,
              ffi.Pointer<ffi.Float>, int)>();

  /// Return 1 if there are no speech segments available.
  /// Return 0 if there are speech segments.
  int SherpaOnnxVoiceActivityDetectorEmpty(
    ffi.Pointer<SherpaOnnxVoiceActivityDetector> p,
  ) {
    return _SherpaOnnxVoiceActivityDetectorEmpty(
      p,
    );
  }

  late final _SherpaOnnxVoiceActivityDetectorEmptyPtr = _lookup<
          ffi.NativeFunction<
              ffi.Int32 Function(
                  ffi.Pointer<SherpaOnnxVoiceActivityDetector>)>>(
      'SherpaOnnxVoiceActivityDetectorEmpty');
  late final _SherpaOnnxVoiceActivityDetectorEmpty =
      _SherpaOnnxVoiceActivityDetectorEmptyPtr.asFunction<
          int Function(ffi.Pointer<SherpaOnnxVoiceActivityDetector>)>();

  /// Return 1 if there is voice detected.
  /// Return 0 if voice is silent.
  int SherpaOnnxVoiceActivityDetectorDetected(
    ffi.Pointer<SherpaOnnxVoiceActivityDetector> p,
  ) {
    return _SherpaOnnxVoiceActivityDetectorDetected(
      p,
    );
  }

  late final _SherpaOnnxVoiceActivityDetectorDetectedPtr = _lookup<
          ffi.NativeFunction<
              ffi.Int32 Function(
                  ffi.Pointer<SherpaOnnxVoiceActivityDetector>)>>(
      'SherpaOnnxVoiceActivityDetectorDetected');
  late final _SherpaOnnxVoiceActivityDetectorDetected =
      _SherpaOnnxVoiceActivityDetectorDetectedPtr.asFunction<
          int Function(ffi.Pointer<SherpaOnnxVoiceActivityDetector>)>();

  /// Return the first speech segment.
  /// It throws if SherpaOnnxVoiceActivityDetectorEmpty() returns 1.
  void SherpaOnnxVoiceActivityDetectorPop(
    ffi.Pointer<SherpaOnnxVoiceActivityDetector> p,
  ) {
    return _SherpaOnnxVoiceActivityDetectorPop(
      p,
    );
  }

  late final _SherpaOnnxVoiceActivityDetectorPopPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<SherpaOnnxVoiceActivityDetector>)>>(
      'SherpaOnnxVoiceActivityDetectorPop');
  late final _SherpaOnnxVoiceActivityDetectorPop =
      _SherpaOnnxVoiceActivityDetectorPopPtr.asFunction<
          void Function(ffi.Pointer<SherpaOnnxVoiceActivityDetector>)>();

  /// Clear current speech segments.
  void SherpaOnnxVoiceActivityDetectorClear(
    ffi.Pointer<SherpaOnnxVoiceActivityDetector> p,
  ) {
    return _SherpaOnnxVoiceActivityDetectorClear(
      p,
    );
  }

  late final _SherpaOnnxVoiceActivityDetectorClearPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<SherpaOnnxVoiceActivityDetector>)>>(
      'SherpaOnnxVoiceActivityDetectorClear');
  late final _SherpaOnnxVoiceActivityDetectorClear =
      _SherpaOnnxVoiceActivityDetectorClearPtr.asFunction<
          void Function(ffi.Pointer<SherpaOnnxVoiceActivityDetector>)>();

  /// Return the first speech segment.
  /// The user has to use SherpaOnnxDestroySpeechSegment() to free the returned
  /// pointer to avoid memory leak.
  ffi.Pointer<SherpaOnnxSpeechSegment> SherpaOnnxVoiceActivityDetectorFront(
    ffi.Pointer<SherpaOnnxVoiceActivityDetector> p,
  ) {
    return _SherpaOnnxVoiceActivityDetectorFront(
      p,
    );
  }

  late final _SherpaOnnxVoiceActivityDetectorFrontPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<SherpaOnnxSpeechSegment> Function(
                  ffi.Pointer<SherpaOnnxVoiceActivityDetector>)>>(
      'SherpaOnnxVoiceActivityDetectorFront');
  late final _SherpaOnnxVoiceActivityDetectorFront =
      _SherpaOnnxVoiceActivityDetectorFrontPtr.asFunction<
          ffi.Pointer<SherpaOnnxSpeechSegment> Function(
              ffi.Pointer<SherpaOnnxVoiceActivityDetector>)>();

  /// Free the pointer returned SherpaOnnxVoiceActivityDetectorFront().
  void SherpaOnnxDestroySpeechSegment(
    ffi.Pointer<SherpaOnnxSpeechSegment> p,
  ) {
    return _SherpaOnnxDestroySpeechSegment(
      p,
    );
  }

  late final _SherpaOnnxDestroySpeechSegmentPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<SherpaOnnxSpeechSegment>)>>(
      'SherpaOnnxDestroySpeechSegment');
  late final _SherpaOnnxDestroySpeechSegment =
      _SherpaOnnxDestroySpeechSegmentPtr.asFunction<
          void Function(ffi.Pointer<SherpaOnnxSpeechSegment>)>();

  /// Re-initialize the voice activity detector.
  void SherpaOnnxVoiceActivityDetectorReset(
    ffi.Pointer<SherpaOnnxVoiceActivityDetector> p,
  ) {
    return _SherpaOnnxVoiceActivityDetectorReset(
      p,
    );
  }

  late final _SherpaOnnxVoiceActivityDetectorResetPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<SherpaOnnxVoiceActivityDetector>)>>(
      'SherpaOnnxVoiceActivityDetectorReset');
  late final _SherpaOnnxVoiceActivityDetectorReset =
      _SherpaOnnxVoiceActivityDetectorResetPtr.asFunction<
          void Function(ffi.Pointer<SherpaOnnxVoiceActivityDetector>)>();

  /// Create an instance of offline TTS. The user has to use DestroyOfflineTts()
  /// to free the returned pointer to avoid memory leak.
  ffi.Pointer<SherpaOnnxOfflineTts> SherpaOnnxCreateOfflineTts(
    ffi.Pointer<SherpaOnnxOfflineTtsConfig> config,
  ) {
    return _SherpaOnnxCreateOfflineTts(
      config,
    );
  }

  late final _SherpaOnnxCreateOfflineTtsPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<SherpaOnnxOfflineTts> Function(
                  ffi.Pointer<SherpaOnnxOfflineTtsConfig>)>>(
      'SherpaOnnxCreateOfflineTts');
  late final _SherpaOnnxCreateOfflineTts =
      _SherpaOnnxCreateOfflineTtsPtr.asFunction<
          ffi.Pointer<SherpaOnnxOfflineTts> Function(
              ffi.Pointer<SherpaOnnxOfflineTtsConfig>)>();

  /// Free the pointer returned by CreateOfflineTts()
  void SherpaOnnxDestroyOfflineTts(
    ffi.Pointer<SherpaOnnxOfflineTts> tts,
  ) {
    return _SherpaOnnxDestroyOfflineTts(
      tts,
    );
  }

  late final _SherpaOnnxDestroyOfflineTtsPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<SherpaOnnxOfflineTts>)>>(
      'SherpaOnnxDestroyOfflineTts');
  late final _SherpaOnnxDestroyOfflineTts = _SherpaOnnxDestroyOfflineTtsPtr
      .asFunction<void Function(ffi.Pointer<SherpaOnnxOfflineTts>)>();

  /// Return the sample rate of the current TTS object
  int SherpaOnnxOfflineTtsSampleRate(
    ffi.Pointer<SherpaOnnxOfflineTts> tts,
  ) {
    return _SherpaOnnxOfflineTtsSampleRate(
      tts,
    );
  }

  late final _SherpaOnnxOfflineTtsSampleRatePtr = _lookup<
          ffi.NativeFunction<
              ffi.Int32 Function(ffi.Pointer<SherpaOnnxOfflineTts>)>>(
      'SherpaOnnxOfflineTtsSampleRate');
  late final _SherpaOnnxOfflineTtsSampleRate =
      _SherpaOnnxOfflineTtsSampleRatePtr.asFunction<
          int Function(ffi.Pointer<SherpaOnnxOfflineTts>)>();

  /// Generate audio from the given text and speaker id (sid).
  /// The user has to use DestroyOfflineTtsGeneratedAudio() to free the
  /// returned pointer to avoid memory leak.
  ffi.Pointer<SherpaOnnxGeneratedAudio> SherpaOnnxOfflineTtsGenerate(
    ffi.Pointer<SherpaOnnxOfflineTts> tts,
    ffi.Pointer<ffi.Char> text,
    int sid,
    double speed,
  ) {
    return _SherpaOnnxOfflineTtsGenerate(
      tts,
      text,
      sid,
      speed,
    );
  }

  late final _SherpaOnnxOfflineTtsGeneratePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<SherpaOnnxGeneratedAudio> Function(
              ffi.Pointer<SherpaOnnxOfflineTts>,
              ffi.Pointer<ffi.Char>,
              ffi.Int32,
              ffi.Float)>>('SherpaOnnxOfflineTtsGenerate');
  late final _SherpaOnnxOfflineTtsGenerate =
      _SherpaOnnxOfflineTtsGeneratePtr.asFunction<
          ffi.Pointer<SherpaOnnxGeneratedAudio> Function(
              ffi.Pointer<SherpaOnnxOfflineTts>,
              ffi.Pointer<ffi.Char>,
              int,
              double)>();

  /// callback is called whenever SherpaOnnxOfflineTtsConfig.max_num_sentences
  /// sentences have been processed. The pointer passed to the callback
  /// is freed once the callback is returned. So the caller should not keep
  /// a reference to it.
  ffi.Pointer<SherpaOnnxGeneratedAudio>
      SherpaOnnxOfflineTtsGenerateWithCallback(
    ffi.Pointer<SherpaOnnxOfflineTts> tts,
    ffi.Pointer<ffi.Char> text,
    int sid,
    double speed,
    SherpaOnnxGeneratedAudioCallback callback,
  ) {
    return _SherpaOnnxOfflineTtsGenerateWithCallback(
      tts,
      text,
      sid,
      speed,
      callback,
    );
  }

  late final _SherpaOnnxOfflineTtsGenerateWithCallbackPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<SherpaOnnxGeneratedAudio> Function(
                  ffi.Pointer<SherpaOnnxOfflineTts>,
                  ffi.Pointer<ffi.Char>,
                  ffi.Int32,
                  ffi.Float,
                  SherpaOnnxGeneratedAudioCallback)>>(
      'SherpaOnnxOfflineTtsGenerateWithCallback');
  late final _SherpaOnnxOfflineTtsGenerateWithCallback =
      _SherpaOnnxOfflineTtsGenerateWithCallbackPtr.asFunction<
          ffi.Pointer<SherpaOnnxGeneratedAudio> Function(
              ffi.Pointer<SherpaOnnxOfflineTts>,
              ffi.Pointer<ffi.Char>,
              int,
              double,
              SherpaOnnxGeneratedAudioCallback)>();

  void SherpaOnnxDestroyOfflineTtsGeneratedAudio(
    ffi.Pointer<SherpaOnnxGeneratedAudio> p,
  ) {
    return _SherpaOnnxDestroyOfflineTtsGeneratedAudio(
      p,
    );
  }

  late final _SherpaOnnxDestroyOfflineTtsGeneratedAudioPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(ffi.Pointer<SherpaOnnxGeneratedAudio>)>>(
      'SherpaOnnxDestroyOfflineTtsGeneratedAudio');
  late final _SherpaOnnxDestroyOfflineTtsGeneratedAudio =
      _SherpaOnnxDestroyOfflineTtsGeneratedAudioPtr.asFunction<
          void Function(ffi.Pointer<SherpaOnnxGeneratedAudio>)>();

  /// Write the generated audio to a wave file.
  /// The saved wave file contains a single channel and has 16-bit samples.
  ///
  /// Return 1 if the write succeeded; return 0 on failure.
  int SherpaOnnxWriteWave(
    ffi.Pointer<ffi.Float> samples,
    int n,
    int sample_rate,
    ffi.Pointer<ffi.Char> filename,
  ) {
    return _SherpaOnnxWriteWave(
      samples,
      n,
      sample_rate,
      filename,
    );
  }

  late final _SherpaOnnxWriteWavePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<ffi.Float>, ffi.Int32, ffi.Int32,
              ffi.Pointer<ffi.Char>)>>('SherpaOnnxWriteWave');
  late final _SherpaOnnxWriteWave = _SherpaOnnxWriteWavePtr.asFunction<
      int Function(ffi.Pointer<ffi.Float>, int, int, ffi.Pointer<ffi.Char>)>();
}

/// C interface
///
/// TODO: show sample usage
final class llama_model extends ffi.Opaque {}

final class llama_context extends ffi.Opaque {}

abstract class llama_vocab_type {
  /// SentencePiece
  static const int LLAMA_VOCAB_TYPE_SPM = 0;

  /// Byte Pair Encoding
  static const int LLAMA_VOCAB_TYPE_BPE = 1;

  /// WordPiece
  static const int LLAMA_VOCAB_TYPE_WPM = 2;
}

/// note: these values should be synchronized with ggml_rope
/// TODO: maybe move this enum to ggml.h (ggml_rope_type)
abstract class llama_rope_type {
  static const int LLAMA_ROPE_TYPE_NONE = -1;
  static const int LLAMA_ROPE_TYPE_NORM = 0;
  static const int LLAMA_ROPE_TYPE_NEOX = 2;
  static const int LLAMA_ROPE_TYPE_GLM = 4;
}

abstract class llama_token_type {
  static const int LLAMA_TOKEN_TYPE_UNDEFINED = 0;
  static const int LLAMA_TOKEN_TYPE_NORMAL = 1;
  static const int LLAMA_TOKEN_TYPE_UNKNOWN = 2;
  static const int LLAMA_TOKEN_TYPE_CONTROL = 3;
  static const int LLAMA_TOKEN_TYPE_USER_DEFINED = 4;
  static const int LLAMA_TOKEN_TYPE_UNUSED = 5;
  static const int LLAMA_TOKEN_TYPE_BYTE = 6;
}

/// model file types
abstract class llama_ftype {
  static const int LLAMA_FTYPE_ALL_F32 = 0;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_F16 = 1;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_0 = 2;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_1 = 3;

  /// tok_embeddings.weight and output.weight are F16
  static const int LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16 = 4;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q8_0 = 7;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q5_0 = 8;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q5_1 = 9;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q2_K = 10;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q3_K_S = 11;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q3_K_M = 12;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q3_K_L = 13;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_K_S = 14;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_K_M = 15;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q5_K_S = 16;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q5_K_M = 17;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q6_K = 18;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ2_XXS = 19;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ2_XS = 20;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q2_K_S = 21;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ3_XS = 22;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ3_XXS = 23;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ1_S = 24;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ4_NL = 25;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ3_S = 26;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ3_M = 27;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ2_S = 28;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ2_M = 29;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ4_XS = 30;

  /// not specified in the model file
  static const int LLAMA_FTYPE_GUESSED = 1024;
}

abstract class llama_rope_scaling_type {
  static const int LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED = -1;
  static const int LLAMA_ROPE_SCALING_TYPE_NONE = 0;
  static const int LLAMA_ROPE_SCALING_TYPE_LINEAR = 1;
  static const int LLAMA_ROPE_SCALING_TYPE_YARN = 2;
  static const int LLAMA_ROPE_SCALING_TYPE_MAX_VALUE = 2;
}

abstract class llama_pooling_type {
  static const int LLAMA_POOLING_TYPE_UNSPECIFIED = -1;
  static const int LLAMA_POOLING_TYPE_NONE = 0;
  static const int LLAMA_POOLING_TYPE_MEAN = 1;
  static const int LLAMA_POOLING_TYPE_CLS = 2;
}

abstract class llama_split_mode {
  /// single GPU
  static const int LLAMA_SPLIT_MODE_NONE = 0;

  /// split layers and KV across GPUs
  static const int LLAMA_SPLIT_MODE_LAYER = 1;

  /// split rows across GPUs
  static const int LLAMA_SPLIT_MODE_ROW = 2;
}

final class llama_token_data extends ffi.Struct {
  /// token id
  @llama_token()
  external int id;

  /// log-odds of the token
  @ffi.Float()
  external double logit;

  /// probability of the token
  @ffi.Float()
  external double p;
}

typedef llama_token = ffi.Int32;
typedef Dartllama_token = int;

final class llama_token_data_array extends ffi.Struct {
  external ffi.Pointer<llama_token_data> data;

  @ffi.Size()
  external int size;

  @ffi.Bool()
  external bool sorted;
}

/// Input data for llama_decode
/// A llama_batch object can contain input about one or many sequences
/// The provided arrays (i.e. token, embd, pos, etc.) must have size of n_tokens
///
/// - token  : the token ids of the input (used when embd is NULL)
/// - embd   : token embeddings (i.e. float vector of size n_embd) (used when token is NULL)
/// - pos    : the positions of the respective token in the sequence
/// - seq_id : the sequence to which the respective token belongs
/// - logits : if zero, the logits for the respective token will not be output
final class llama_batch extends ffi.Struct {
  @ffi.Int32()
  external int n_tokens;

  external ffi.Pointer<llama_token> token;

  external ffi.Pointer<ffi.Float> embd;

  external ffi.Pointer<llama_pos> pos;

  external ffi.Pointer<ffi.Int32> n_seq_id;

  external ffi.Pointer<ffi.Pointer<llama_seq_id>> seq_id;

  external ffi.Pointer<ffi.Int8> logits;

  /// used if pos == NULL
  @llama_pos()
  external int all_pos_0;

  /// used if pos == NULL
  @llama_pos()
  external int all_pos_1;

  /// used if seq_id == NULL
  @llama_seq_id()
  external int all_seq_id;
}

typedef llama_pos = ffi.Int32;
typedef Dartllama_pos = int;
typedef llama_seq_id = ffi.Int32;
typedef Dartllama_seq_id = int;

abstract class llama_model_kv_override_type {
  static const int LLAMA_KV_OVERRIDE_TYPE_INT = 0;
  static const int LLAMA_KV_OVERRIDE_TYPE_FLOAT = 1;
  static const int LLAMA_KV_OVERRIDE_TYPE_BOOL = 2;
}

final class llama_model_kv_override extends ffi.Struct {
  @ffi.Array.multi([128])
  external ffi.Array<ffi.Char> key;

  @ffi.Int32()
  external int tag;

  external UnnamedUnion1 unnamed;
}

final class UnnamedUnion1 extends ffi.Union {
  @ffi.Int64()
  external int int_value;

  @ffi.Double()
  external double float_value;

  @ffi.Bool()
  external bool bool_value;
}

final class llama_model_params extends ffi.Struct {
  /// number of layers to store in VRAM
  @ffi.Int32()
  external int n_gpu_layers;

  /// how to split the model across multiple GPUs
  @ffi.Int32()
  external int split_mode;

  /// main_gpu interpretation depends on split_mode:
  /// LLAMA_SPLIT_NONE: the GPU that is used for the entire model
  /// LLAMA_SPLIT_ROW: the GPU that is used for small tensors and intermediate results
  /// LLAMA_SPLIT_LAYER: ignored
  @ffi.Int32()
  external int main_gpu;

  /// proportion of the model (layers or rows) to offload to each GPU, size: llama_max_devices()
  external ffi.Pointer<ffi.Float> tensor_split;

  /// Called with a progress value between 0.0 and 1.0. Pass NULL to disable.
  /// If the provided progress_callback returns true, model loading continues.
  /// If it returns false, model loading is immediately aborted.
  external llama_progress_callback progress_callback;

  /// context pointer passed to the progress callback
  external ffi.Pointer<ffi.Void> progress_callback_user_data;

  /// override key-value pairs of the model meta data
  external ffi.Pointer<llama_model_kv_override> kv_overrides;

  /// only load the vocabulary, no weights
  @ffi.Bool()
  external bool vocab_only;

  /// use mmap if possible
  @ffi.Bool()
  external bool use_mmap;

  /// force system to keep model in RAM
  @ffi.Bool()
  external bool use_mlock;
}

typedef llama_progress_callback
    = ffi.Pointer<ffi.NativeFunction<llama_progress_callbackFunction>>;
typedef llama_progress_callbackFunction = ffi.Bool Function(
    ffi.Float progress, ffi.Pointer<ffi.Void> ctx);
typedef Dartllama_progress_callbackFunction = bool Function(
    double progress, ffi.Pointer<ffi.Void> ctx);

final class llama_context_params extends ffi.Struct {
  /// RNG seed, -1 for random
  @ffi.Uint32()
  external int seed;

  /// text context, 0 = from model
  @ffi.Uint32()
  external int n_ctx;

  /// prompt processing maximum batch size
  @ffi.Uint32()
  external int n_batch;

  /// number of threads to use for generation
  @ffi.Uint32()
  external int n_threads;

  /// number of threads to use for batch processing
  @ffi.Uint32()
  external int n_threads_batch;

  /// RoPE scaling type, from `enum llama_rope_scaling_type`
  @ffi.Int32()
  external int rope_scaling_type;

  /// whether to pool (sum) embedding results by sequence id
  /// (ignored if no pooling layer)
  @ffi.Int32()
  external int pooling_type;

  /// RoPE base frequency, 0 = from model
  @ffi.Float()
  external double rope_freq_base;

  /// RoPE frequency scaling factor, 0 = from model
  @ffi.Float()
  external double rope_freq_scale;

  /// YaRN extrapolation mix factor, negative = from model
  @ffi.Float()
  external double yarn_ext_factor;

  /// YaRN magnitude scaling factor
  @ffi.Float()
  external double yarn_attn_factor;

  /// YaRN low correction dim
  @ffi.Float()
  external double yarn_beta_fast;

  /// YaRN high correction dim
  @ffi.Float()
  external double yarn_beta_slow;

  /// YaRN original context size
  @ffi.Uint32()
  external int yarn_orig_ctx;

  /// defragment the KV cache if holes/size > thold, < 0 disabled (default)
  @ffi.Float()
  external double defrag_thold;

  external ggml_backend_sched_eval_callback cb_eval;

  external ffi.Pointer<ffi.Void> cb_eval_user_data;

  /// data type for K cache
  @ffi.Int32()
  external int type_k;

  /// data type for V cache
  @ffi.Int32()
  external int type_v;

  /// the llama_decode() call computes all logits, not just the last one (DEPRECATED - set llama_batch.logits instead)
  @ffi.Bool()
  external bool logits_all;

  /// embedding mode only
  @ffi.Bool()
  external bool embedding;

  /// whether to offload the KQV ops (including the KV cache) to GPU
  @ffi.Bool()
  external bool offload_kqv;

  /// Abort callback
  /// if it returns true, execution of llama_decode() will be aborted
  /// currently works only with CPU execution
  external ggml_abort_callback abort_callback;

  external ffi.Pointer<ffi.Void> abort_callback_data;
}

/// when ask == true, the scheduler wants to know if the user wants to observe this node
/// this allows the scheduler to batch nodes together in order to evaluate them in a single call
///
/// when ask == false, the scheduler is passing the node tensor to the user for observation
/// if the user returns false, the scheduler will cancel the graph compute
typedef ggml_backend_sched_eval_callback
    = ffi.Pointer<ffi.NativeFunction<ggml_backend_sched_eval_callbackFunction>>;
typedef ggml_backend_sched_eval_callbackFunction = ffi.Bool Function(
    ffi.Pointer<ggml_tensor> t, ffi.Bool ask, ffi.Pointer<ffi.Void> user_data);
typedef Dartggml_backend_sched_eval_callbackFunction = bool Function(
    ffi.Pointer<ggml_tensor> t, bool ask, ffi.Pointer<ffi.Void> user_data);

/// n-dimensional tensor
final class ggml_tensor extends ffi.Struct {
  @ffi.Int32()
  external int type;

  @ffi.Int32()
  external int backend;

  external ffi.Pointer<ggml_backend_buffer> buffer;

  /// number of elements
  @ffi.Array.multi([4])
  external ffi.Array<ffi.Int64> ne;

  /// stride in bytes:
  /// nb[0] = ggml_type_size(type)
  /// nb[1] = nb[0]   * (ne[0] / ggml_blck_size(type)) + padding
  /// nb[i] = nb[i-1] * ne[i-1]
  @ffi.Array.multi([4])
  external ffi.Array<ffi.Size> nb;

  /// compute data
  @ffi.Int32()
  external int op;

  /// op params - allocated as int32_t for alignment
  @ffi.Array.multi([16])
  external ffi.Array<ffi.Int32> op_params;

  @ffi.Int32()
  external int flags;

  external ffi.Pointer<ggml_tensor> grad;

  @ffi.Array.multi([10])
  external ffi.Array<ffi.Pointer<ggml_tensor>> src;

  /// performance
  @ffi.Int()
  external int perf_runs;

  @ffi.Int64()
  external int perf_cycles;

  @ffi.Int64()
  external int perf_time_us;

  external ffi.Pointer<ggml_tensor> view_src;

  @ffi.Size()
  external int view_offs;

  external ffi.Pointer<ffi.Void> data;

  @ffi.Array.multi([64])
  external ffi.Array<ffi.Char> name;

  /// extra things e.g. for ggml-cuda.cu
  external ffi.Pointer<ffi.Void> extra;

  @ffi.Array.multi([8])
  external ffi.Array<ffi.Char> padding;
}

abstract class ggml_type {
  static const int GGML_TYPE_F32 = 0;
  static const int GGML_TYPE_F16 = 1;
  static const int GGML_TYPE_Q4_0 = 2;
  static const int GGML_TYPE_Q4_1 = 3;

  /// GGML_TYPE_Q4_2 = 4, support has been removed
  /// GGML_TYPE_Q4_3 (5) support has been removed
  static const int GGML_TYPE_Q5_0 = 6;
  static const int GGML_TYPE_Q5_1 = 7;
  static const int GGML_TYPE_Q8_0 = 8;
  static const int GGML_TYPE_Q8_1 = 9;

  /// k-quantizations
  static const int GGML_TYPE_Q2_K = 10;
  static const int GGML_TYPE_Q3_K = 11;
  static const int GGML_TYPE_Q4_K = 12;
  static const int GGML_TYPE_Q5_K = 13;
  static const int GGML_TYPE_Q6_K = 14;
  static const int GGML_TYPE_Q8_K = 15;
  static const int GGML_TYPE_IQ2_XXS = 16;
  static const int GGML_TYPE_IQ2_XS = 17;
  static const int GGML_TYPE_IQ3_XXS = 18;
  static const int GGML_TYPE_IQ1_S = 19;
  static const int GGML_TYPE_IQ4_NL = 20;
  static const int GGML_TYPE_IQ3_S = 21;
  static const int GGML_TYPE_IQ2_S = 22;
  static const int GGML_TYPE_IQ4_XS = 23;
  static const int GGML_TYPE_I8 = 24;
  static const int GGML_TYPE_I16 = 25;
  static const int GGML_TYPE_I32 = 26;
  static const int GGML_TYPE_COUNT = 27;
}

abstract class ggml_backend_type {
  static const int GGML_BACKEND_TYPE_CPU = 0;
  static const int GGML_BACKEND_TYPE_GPU = 10;
  static const int GGML_BACKEND_TYPE_GPU_SPLIT = 20;
}

final class ggml_backend_buffer extends ffi.Opaque {}

/// available tensor operations:
abstract class ggml_op {
  static const int GGML_OP_NONE = 0;
  static const int GGML_OP_DUP = 1;
  static const int GGML_OP_ADD = 2;
  static const int GGML_OP_ADD1 = 3;
  static const int GGML_OP_ACC = 4;
  static const int GGML_OP_SUB = 5;
  static const int GGML_OP_MUL = 6;
  static const int GGML_OP_DIV = 7;
  static const int GGML_OP_SQR = 8;
  static const int GGML_OP_SQRT = 9;
  static const int GGML_OP_LOG = 10;
  static const int GGML_OP_SUM = 11;
  static const int GGML_OP_SUM_ROWS = 12;
  static const int GGML_OP_MEAN = 13;
  static const int GGML_OP_ARGMAX = 14;
  static const int GGML_OP_REPEAT = 15;
  static const int GGML_OP_REPEAT_BACK = 16;
  static const int GGML_OP_CONCAT = 17;
  static const int GGML_OP_SILU_BACK = 18;

  /// normalize
  static const int GGML_OP_NORM = 19;
  static const int GGML_OP_RMS_NORM = 20;
  static const int GGML_OP_RMS_NORM_BACK = 21;
  static const int GGML_OP_GROUP_NORM = 22;
  static const int GGML_OP_MUL_MAT = 23;
  static const int GGML_OP_MUL_MAT_ID = 24;
  static const int GGML_OP_OUT_PROD = 25;
  static const int GGML_OP_SCALE = 26;
  static const int GGML_OP_SET = 27;
  static const int GGML_OP_CPY = 28;
  static const int GGML_OP_CONT = 29;
  static const int GGML_OP_RESHAPE = 30;
  static const int GGML_OP_VIEW = 31;
  static const int GGML_OP_PERMUTE = 32;
  static const int GGML_OP_TRANSPOSE = 33;
  static const int GGML_OP_GET_ROWS = 34;
  static const int GGML_OP_GET_ROWS_BACK = 35;
  static const int GGML_OP_DIAG = 36;
  static const int GGML_OP_DIAG_MASK_INF = 37;
  static const int GGML_OP_DIAG_MASK_ZERO = 38;
  static const int GGML_OP_SOFT_MAX = 39;
  static const int GGML_OP_SOFT_MAX_BACK = 40;
  static const int GGML_OP_ROPE = 41;
  static const int GGML_OP_ROPE_BACK = 42;
  static const int GGML_OP_ALIBI = 43;
  static const int GGML_OP_CLAMP = 44;
  static const int GGML_OP_CONV_TRANSPOSE_1D = 45;
  static const int GGML_OP_IM2COL = 46;
  static const int GGML_OP_CONV_TRANSPOSE_2D = 47;
  static const int GGML_OP_POOL_1D = 48;
  static const int GGML_OP_POOL_2D = 49;

  /// nearest interpolate
  static const int GGML_OP_UPSCALE = 50;
  static const int GGML_OP_PAD = 51;
  static const int GGML_OP_ARGSORT = 52;
  static const int GGML_OP_LEAKY_RELU = 53;
  static const int GGML_OP_FLASH_ATTN = 54;
  static const int GGML_OP_FLASH_FF = 55;
  static const int GGML_OP_FLASH_ATTN_BACK = 56;
  static const int GGML_OP_WIN_PART = 57;
  static const int GGML_OP_WIN_UNPART = 58;
  static const int GGML_OP_GET_REL_POS = 59;
  static const int GGML_OP_ADD_REL_POS = 60;
  static const int GGML_OP_UNARY = 61;
  static const int GGML_OP_MAP_UNARY = 62;
  static const int GGML_OP_MAP_BINARY = 63;
  static const int GGML_OP_MAP_CUSTOM1_F32 = 64;
  static const int GGML_OP_MAP_CUSTOM2_F32 = 65;
  static const int GGML_OP_MAP_CUSTOM3_F32 = 66;
  static const int GGML_OP_MAP_CUSTOM1 = 67;
  static const int GGML_OP_MAP_CUSTOM2 = 68;
  static const int GGML_OP_MAP_CUSTOM3 = 69;
  static const int GGML_OP_CROSS_ENTROPY_LOSS = 70;
  static const int GGML_OP_CROSS_ENTROPY_LOSS_BACK = 71;
  static const int GGML_OP_COUNT = 72;
}

/// Abort callback
/// If not NULL, called before ggml computation
/// If it returns true, the computation is aborted
typedef ggml_abort_callback
    = ffi.Pointer<ffi.NativeFunction<ggml_abort_callbackFunction>>;
typedef ggml_abort_callbackFunction = ffi.Bool Function(
    ffi.Pointer<ffi.Void> data);
typedef Dartggml_abort_callbackFunction = bool Function(
    ffi.Pointer<ffi.Void> data);

/// model quantization parameters
final class llama_model_quantize_params extends ffi.Struct {
  /// number of threads to use for quantizing, if <=0 will use std::thread::hardware_concurrency()
  @ffi.Int32()
  external int nthread;

  /// quantize to this llama_ftype
  @ffi.Int32()
  external int ftype;

  /// allow quantizing non-f32/f16 tensors
  @ffi.Bool()
  external bool allow_requantize;

  /// quantize output.weight
  @ffi.Bool()
  external bool quantize_output_tensor;

  /// only copy tensors - ftype, allow_requantize and quantize_output_tensor are ignored
  @ffi.Bool()
  external bool only_copy;

  /// disable k-quant mixtures and quantize all tensors to the same type
  @ffi.Bool()
  external bool pure;

  /// pointer to importance matrix data
  external ffi.Pointer<ffi.Void> imatrix;
}

/// grammar types
final class llama_grammar extends ffi.Opaque {}

/// grammar element type
abstract class llama_gretype {
  /// end of rule definition
  static const int LLAMA_GRETYPE_END = 0;

  /// start of alternate definition for rule
  static const int LLAMA_GRETYPE_ALT = 1;

  /// non-terminal element: reference to rule
  static const int LLAMA_GRETYPE_RULE_REF = 2;

  /// terminal element: character (code point)
  static const int LLAMA_GRETYPE_CHAR = 3;

  /// inverse char(s) ([^a], [^a-b] [^abc])
  static const int LLAMA_GRETYPE_CHAR_NOT = 4;

  /// modifies a preceding LLAMA_GRETYPE_CHAR or LLAMA_GRETYPE_CHAR_ALT to
  /// be an inclusive range ([a-z])
  static const int LLAMA_GRETYPE_CHAR_RNG_UPPER = 5;

  /// modifies a preceding LLAMA_GRETYPE_CHAR or
  /// LLAMA_GRETYPE_CHAR_RNG_UPPER to add an alternate char to match ([ab], [a-zA])
  static const int LLAMA_GRETYPE_CHAR_ALT = 6;
}

final class llama_grammar_element extends ffi.Struct {
  @ffi.Int32()
  external int type;

  /// Unicode code point or rule ID
  @ffi.Uint32()
  external int value;
}

/// performance timing information
final class llama_timings extends ffi.Struct {
  @ffi.Double()
  external double t_start_ms;

  @ffi.Double()
  external double t_end_ms;

  @ffi.Double()
  external double t_load_ms;

  @ffi.Double()
  external double t_sample_ms;

  @ffi.Double()
  external double t_p_eval_ms;

  @ffi.Double()
  external double t_eval_ms;

  @ffi.Int32()
  external int n_sample;

  @ffi.Int32()
  external int n_p_eval;

  @ffi.Int32()
  external int n_eval;
}

/// used in chat template
final class llama_chat_message extends ffi.Struct {
  external ffi.Pointer<ffi.Char> role;

  external ffi.Pointer<ffi.Char> content;
}

/// numa strategies
abstract class ggml_numa_strategy {
  static const int GGML_NUMA_STRATEGY_DISABLED = 0;
  static const int GGML_NUMA_STRATEGY_DISTRIBUTE = 1;
  static const int GGML_NUMA_STRATEGY_ISOLATE = 2;
  static const int GGML_NUMA_STRATEGY_NUMACTL = 3;
  static const int GGML_NUMA_STRATEGY_MIRROR = 4;
  static const int GGML_NUMA_STRATEGY_COUNT = 5;
}

/// Information associated with an individual cell in the KV cache view.
final class llama_kv_cache_view_cell extends ffi.Struct {
  /// The position for this cell. Takes KV cache shifts into account.
  /// May be negative if the cell is not populated.
  @llama_pos()
  external int pos;
}

/// An updateable view of the KV cache.
final class llama_kv_cache_view extends ffi.Struct {
  /// Number of KV cache cells. This will be the same as the context size.
  @ffi.Int32()
  external int n_cells;

  /// Maximum number of sequences that can exist in a cell. It's not an error
  /// if there are more sequences in a cell than this value, however they will
  /// not be visible in the view cells_sequences.
  @ffi.Int32()
  external int n_max_seq;

  /// Number of tokens in the cache. For example, if there are two populated
  /// cells, the first with 1 sequence id in it and the second with 2 sequence
  /// ids then you'll have 3 tokens.
  @ffi.Int32()
  external int token_count;

  /// Number of populated cache cells.
  @ffi.Int32()
  external int used_cells;

  /// Maximum contiguous empty slots in the cache.
  @ffi.Int32()
  external int max_contiguous;

  /// Index to the start of the max_contiguous slot range. Can be negative
  /// when cache is full.
  @ffi.Int32()
  external int max_contiguous_idx;

  /// Information for an individual cell.
  external ffi.Pointer<llama_kv_cache_view_cell> cells;

  /// The sequences for each cell. There will be n_max_seq items per cell.
  external ffi.Pointer<llama_seq_id> cells_sequences;
}

/// Beam search
final class llama_beam_view extends ffi.Struct {
  external ffi.Pointer<llama_token> tokens;

  @ffi.Size()
  external int n_tokens;

  /// Cumulative beam probability (renormalized relative to all beams)
  @ffi.Float()
  external double p;

  /// Callback should set this to true when a beam is at end-of-beam.
  @ffi.Bool()
  external bool eob;
}

/// Passed to beam_search_callback function.
/// Whenever 0 < common_prefix_length, this number of tokens should be copied from any of the beams
/// (e.g. beams[0]) as they will be removed (shifted) from all beams in all subsequent callbacks.
/// These pointers are valid only during the synchronous callback, so should not be saved.
final class llama_beams_state extends ffi.Struct {
  external ffi.Pointer<llama_beam_view> beam_views;

  /// Number of elements in beam_views[].
  @ffi.Size()
  external int n_beams;

  /// Current max length of prefix tokens shared by all beams.
  @ffi.Size()
  external int common_prefix_length;

  /// True iff this is the last callback invocation.
  @ffi.Bool()
  external bool last_call;
}

/// Type of pointer to the beam_search_callback function.
/// void* callback_data is any custom data passed to llama_beam_search, that is subsequently
/// passed back to beam_search_callback. This avoids having to use global variables in the callback.
typedef llama_beam_search_callback_fn_t
    = ffi.Pointer<ffi.NativeFunction<llama_beam_search_callback_fn_tFunction>>;
typedef llama_beam_search_callback_fn_tFunction = ffi.Void Function(
    ffi.Pointer<ffi.Void>, llama_beams_state);
typedef Dartllama_beam_search_callback_fn_tFunction = void Function(
    ffi.Pointer<ffi.Void>, llama_beams_state);
typedef ggml_log_callback
    = ffi.Pointer<ffi.NativeFunction<ggml_log_callbackFunction>>;
typedef ggml_log_callbackFunction = ffi.Void Function(ffi.Int32 level,
    ffi.Pointer<ffi.Char> text, ffi.Pointer<ffi.Void> user_data);
typedef Dartggml_log_callbackFunction = void Function(
    int level, ffi.Pointer<ffi.Char> text, ffi.Pointer<ffi.Void> user_data);

abstract class ggml_log_level {
  static const int GGML_LOG_LEVEL_ERROR = 2;
  static const int GGML_LOG_LEVEL_WARN = 3;
  static const int GGML_LOG_LEVEL_INFO = 4;
  static const int GGML_LOG_LEVEL_DEBUG = 5;
}

/// stdio state variables.
///
/// The following always hold:
///
/// if (_flags&(__SLBF|__SWR)) == (__SLBF|__SWR),
/// _lbfsize is -_bf._size, else _lbfsize is 0
/// if _flags&__SRD, _w is 0
/// if _flags&__SWR, _r is 0
///
/// This ensures that the getc and putc macros (or inline functions) never
/// try to write or read from a file that is in `read' or `write' mode.
/// (Moreover, they can, and do, automatically switch from read mode to
/// write mode, and back, on "r+" and "w+" files.)
///
/// _lbfsize is used only to make the inline line-buffered output stream
/// code as compact as possible.
///
/// _ub, _up, and _ur are used when ungetc() pushes back more characters
/// than fit in the current _bf, or when ungetc() pushes back a character
/// that does not match the previous one in _bf.  When this happens,
/// _ub._base becomes non-nil (i.e., a stream has ungetc() data iff
/// _ub._base!=NULL) and _up and _ur save the current values of _p and _r.
///
/// NB: see WARNING above before changing the layout of this structure!
typedef FILE = __sFILE;

/// stdio state variables.
///
/// The following always hold:
///
/// if (_flags&(__SLBF|__SWR)) == (__SLBF|__SWR),
/// _lbfsize is -_bf._size, else _lbfsize is 0
/// if _flags&__SRD, _w is 0
/// if _flags&__SWR, _r is 0
///
/// This ensures that the getc and putc macros (or inline functions) never
/// try to write or read from a file that is in `read' or `write' mode.
/// (Moreover, they can, and do, automatically switch from read mode to
/// write mode, and back, on "r+" and "w+" files.)
///
/// _lbfsize is used only to make the inline line-buffered output stream
/// code as compact as possible.
///
/// _ub, _up, and _ur are used when ungetc() pushes back more characters
/// than fit in the current _bf, or when ungetc() pushes back a character
/// that does not match the previous one in _bf.  When this happens,
/// _ub._base becomes non-nil (i.e., a stream has ungetc() data iff
/// _ub._base!=NULL) and _up and _ur save the current values of _p and _r.
///
/// NB: see WARNING above before changing the layout of this structure!
final class __sFILE extends ffi.Struct {
  /// current position in (some) buffer
  external ffi.Pointer<ffi.UnsignedChar> _p;

  /// read space left for getc()
  @ffi.Int()
  external int _r;

  /// write space left for putc()
  @ffi.Int()
  external int _w;

  /// flags, below; this FILE is free if 0
  @ffi.Short()
  external int _flags;

  /// fileno, if Unix descriptor, else -1
  @ffi.Short()
  external int _file;

  /// the buffer (at least 1 byte, if !NULL)
  external __sbuf _bf;

  /// 0 or -_bf._size, for inline putc
  @ffi.Int()
  external int _lbfsize;

  /// cookie passed to io functions
  external ffi.Pointer<ffi.Void> _cookie;

  external ffi
      .Pointer<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<ffi.Void>)>>
      _close;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int Function(
              ffi.Pointer<ffi.Void>, ffi.Pointer<ffi.Char>, ffi.Int)>> _read;

  external ffi.Pointer<
      ffi.NativeFunction<
          fpos_t Function(ffi.Pointer<ffi.Void>, fpos_t, ffi.Int)>> _seek;

  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Int Function(
              ffi.Pointer<ffi.Void>, ffi.Pointer<ffi.Char>, ffi.Int)>> _write;

  /// ungetc buffer
  external __sbuf _ub;

  /// additions to FILE to not break ABI
  external ffi.Pointer<__sFILEX> _extra;

  /// saved _r when _r is counting ungetc data
  @ffi.Int()
  external int _ur;

  /// guarantee an ungetc() buffer
  @ffi.Array.multi([3])
  external ffi.Array<ffi.UnsignedChar> _ubuf;

  /// guarantee a getc() buffer
  @ffi.Array.multi([1])
  external ffi.Array<ffi.UnsignedChar> _nbuf;

  /// buffer for fgetln()
  external __sbuf _lb;

  /// stat.st_blksize (may be != _bf._size)
  @ffi.Int()
  external int _blksize;

  /// current lseek offset (see WARNING)
  @fpos_t()
  external int _offset;
}

/// stdio buffers
final class __sbuf extends ffi.Struct {
  external ffi.Pointer<ffi.UnsignedChar> _base;

  @ffi.Int()
  external int _size;
}

typedef fpos_t = __darwin_off_t;
typedef __darwin_off_t = __int64_t;
typedef __int64_t = ffi.LongLong;
typedef Dart__int64_t = int;

/// hold a buncha junk that would grow the ABI
final class __sFILEX extends ffi.Opaque {}

/// Please refer to
/// https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index.html
/// to download pre-trained models. That is, you can find encoder-xxx.onnx
/// decoder-xxx.onnx, joiner-xxx.onnx, and tokens.txt for this struct
/// from there.
final class SherpaOnnxOnlineTransducerModelConfig extends ffi.Struct {
  external ffi.Pointer<ffi.Char> encoder;

  external ffi.Pointer<ffi.Char> decoder;

  external ffi.Pointer<ffi.Char> joiner;
}

/// please visit
/// https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-paraformer/index.html
/// to download pre-trained streaming paraformer models
final class SherpaOnnxOnlineParaformerModelConfig extends ffi.Struct {
  external ffi.Pointer<ffi.Char> encoder;

  external ffi.Pointer<ffi.Char> decoder;
}

/// Please visit
/// https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-ctc/zipformer-ctc-models.html#
/// to download pre-trained streaming zipformer2 ctc models
final class SherpaOnnxOnlineZipformer2CtcModelConfig extends ffi.Struct {
  external ffi.Pointer<ffi.Char> model;
}

final class SherpaOnnxOnlineModelConfig extends ffi.Struct {
  external SherpaOnnxOnlineTransducerModelConfig transducer;

  external SherpaOnnxOnlineParaformerModelConfig paraformer;

  external SherpaOnnxOnlineZipformer2CtcModelConfig zipformer2_ctc;

  external ffi.Pointer<ffi.Char> tokens;

  @ffi.Int32()
  external int num_threads;

  external ffi.Pointer<ffi.Char> provider;

  /// true to print debug information of the model
  @ffi.Int32()
  external int debug;

  external ffi.Pointer<ffi.Char> model_type;
}

/// It expects 16 kHz 16-bit single channel wave format.
final class SherpaOnnxFeatureConfig extends ffi.Struct {
  /// Sample rate of the input data. MUST match the one expected
  /// by the model. For instance, it should be 16000 for models provided
  /// by us.
  @ffi.Int32()
  external int sample_rate;

  /// Feature dimension of the model.
  /// For instance, it should be 80 for models provided by us.
  @ffi.Int32()
  external int feature_dim;
}

final class SherpaOnnxOnlineRecognizerConfig extends ffi.Struct {
  external SherpaOnnxFeatureConfig feat_config;

  external SherpaOnnxOnlineModelConfig model_config;

  /// Possible values are: greedy_search, modified_beam_search
  external ffi.Pointer<ffi.Char> decoding_method;

  /// Used only when decoding_method is modified_beam_search
  /// Example value: 4
  @ffi.Int32()
  external int max_active_paths;

  /// 0 to disable endpoint detection.
  /// A non-zero value to enable endpoint detection.
  @ffi.Int32()
  external int enable_endpoint;

  /// An endpoint is detected if trailing silence in seconds is larger than
  /// this value even if nothing has been decoded.
  /// Used only when enable_endpoint is not 0.
  @ffi.Float()
  external double rule1_min_trailing_silence;

  /// An endpoint is detected if trailing silence in seconds is larger than
  /// this value after something that is not blank has been decoded.
  /// Used only when enable_endpoint is not 0.
  @ffi.Float()
  external double rule2_min_trailing_silence;

  /// An endpoint is detected if the utterance in seconds is larger than
  /// this value.
  /// Used only when enable_endpoint is not 0.
  @ffi.Float()
  external double rule3_min_utterance_length;

  /// Path to the hotwords.
  external ffi.Pointer<ffi.Char> hotwords_file;

  /// Bonus score for each token in hotwords.
  @ffi.Float()
  external double hotwords_score;
}

final class SherpaOnnxOnlineRecognizerResult extends ffi.Struct {
  /// Recognized text
  external ffi.Pointer<ffi.Char> text;

  /// Pointer to continuous memory which holds string based tokens
  /// which are separated by \0
  external ffi.Pointer<ffi.Char> tokens;

  /// a pointer array containing the address of the first item in tokens
  external ffi.Pointer<ffi.Pointer<ffi.Char>> tokens_arr;

  /// Pointer to continuous memory which holds timestamps
  external ffi.Pointer<ffi.Float> timestamps;

  /// The number of tokens/timestamps in above pointer
  @ffi.Int32()
  external int count;

  /// Return a json string.
  ///
  /// The returned string contains:
  /// {
  /// "text": "The recognition result",
  /// "tokens": [x, x, x],
  /// "timestamps": [x, x, x],
  /// "segment": x,
  /// "start_time": x,
  /// "is_final": true|false
  /// }
  external ffi.Pointer<ffi.Char> json;
}

final class SherpaOnnxOnlineRecognizer extends ffi.Opaque {}

final class SherpaOnnxOnlineStream extends ffi.Opaque {}

final class SherpaOnnxDisplay extends ffi.Opaque {}

/// Please refer to
/// https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index.html
/// to download pre-trained models. That is, you can find encoder-xxx.onnx
/// decoder-xxx.onnx, and joiner-xxx.onnx for this struct
/// from there.
final class SherpaOnnxOfflineTransducerModelConfig extends ffi.Struct {
  external ffi.Pointer<ffi.Char> encoder;

  external ffi.Pointer<ffi.Char> decoder;

  external ffi.Pointer<ffi.Char> joiner;
}

final class SherpaOnnxOfflineParaformerModelConfig extends ffi.Struct {
  external ffi.Pointer<ffi.Char> model;
}

final class SherpaOnnxOfflineNemoEncDecCtcModelConfig extends ffi.Struct {
  external ffi.Pointer<ffi.Char> model;
}

final class SherpaOnnxOfflineWhisperModelConfig extends ffi.Struct {
  external ffi.Pointer<ffi.Char> encoder;

  external ffi.Pointer<ffi.Char> decoder;
}

final class SherpaOnnxOfflineTdnnModelConfig extends ffi.Struct {
  external ffi.Pointer<ffi.Char> model;
}

final class SherpaOnnxOfflineLMConfig extends ffi.Struct {
  external ffi.Pointer<ffi.Char> model;

  @ffi.Float()
  external double scale;
}

final class SherpaOnnxOfflineModelConfig extends ffi.Struct {
  external SherpaOnnxOfflineTransducerModelConfig transducer;

  external SherpaOnnxOfflineParaformerModelConfig paraformer;

  external SherpaOnnxOfflineNemoEncDecCtcModelConfig nemo_ctc;

  external SherpaOnnxOfflineWhisperModelConfig whisper;

  external SherpaOnnxOfflineTdnnModelConfig tdnn;

  external ffi.Pointer<ffi.Char> tokens;

  @ffi.Int32()
  external int num_threads;

  @ffi.Int32()
  external int debug;

  external ffi.Pointer<ffi.Char> provider;

  external ffi.Pointer<ffi.Char> model_type;
}

final class SherpaOnnxOfflineRecognizerConfig extends ffi.Struct {
  external SherpaOnnxFeatureConfig feat_config;

  external SherpaOnnxOfflineModelConfig model_config;

  external SherpaOnnxOfflineLMConfig lm_config;

  external ffi.Pointer<ffi.Char> decoding_method;

  @ffi.Int32()
  external int max_active_paths;

  /// Path to the hotwords.
  external ffi.Pointer<ffi.Char> hotwords_file;

  /// Bonus score for each token in hotwords.
  @ffi.Float()
  external double hotwords_score;
}

final class SherpaOnnxOfflineRecognizer extends ffi.Opaque {}

final class SherpaOnnxOfflineStream extends ffi.Opaque {}

final class SherpaOnnxOfflineRecognizerResult extends ffi.Struct {
  external ffi.Pointer<ffi.Char> text;

  /// Pointer to continuous memory which holds timestamps
  ///
  /// It is NULL if the model does not support timestamps
  external ffi.Pointer<ffi.Float> timestamps;

  /// number of entries in timestamps
  @ffi.Int32()
  external int count;
}

/// ============================================================
/// For VAD
/// ============================================================
final class SherpaOnnxSileroVadModelConfig extends ffi.Struct {
  /// Path to the silero VAD model
  external ffi.Pointer<ffi.Char> model;

  /// threshold to classify a segment as speech
  ///
  /// If the predicted probability of a segment is larger than this
  /// value, then it is classified as speech.
  @ffi.Float()
  external double threshold;

  /// in seconds
  @ffi.Float()
  external double min_silence_duration;

  /// in seconds
  @ffi.Float()
  external double min_speech_duration;

  @ffi.Int()
  external int window_size;
}

final class SherpaOnnxVadModelConfig extends ffi.Struct {
  external SherpaOnnxSileroVadModelConfig silero_vad;

  @ffi.Int32()
  external int sample_rate;

  @ffi.Int32()
  external int num_threads;

  external ffi.Pointer<ffi.Char> provider;

  @ffi.Int32()
  external int debug;
}

final class SherpaOnnxCircularBuffer extends ffi.Opaque {}

final class SherpaOnnxSpeechSegment extends ffi.Struct {
  /// The start index in samples of this segment
  @ffi.Int32()
  external int start;

  /// pointer to the array containing the samples
  external ffi.Pointer<ffi.Float> samples;

  /// number of samples in this segment
  @ffi.Int32()
  external int n;
}

final class SherpaOnnxVoiceActivityDetector extends ffi.Opaque {}

/// ============================================================
/// For offline Text-to-Speech (i.e., non-streaming TTS)
/// ============================================================
final class SherpaOnnxOfflineTtsVitsModelConfig extends ffi.Struct {
  external ffi.Pointer<ffi.Char> model;

  external ffi.Pointer<ffi.Char> lexicon;

  external ffi.Pointer<ffi.Char> tokens;

  external ffi.Pointer<ffi.Char> data_dir;

  @ffi.Float()
  external double noise_scale;

  @ffi.Float()
  external double noise_scale_w;

  /// < 1, faster in speed; > 1, slower in speed
  @ffi.Float()
  external double length_scale;
}

final class SherpaOnnxOfflineTtsModelConfig extends ffi.Struct {
  external SherpaOnnxOfflineTtsVitsModelConfig vits;

  @ffi.Int32()
  external int num_threads;

  @ffi.Int32()
  external int debug;

  external ffi.Pointer<ffi.Char> provider;
}

final class SherpaOnnxOfflineTtsConfig extends ffi.Struct {
  external SherpaOnnxOfflineTtsModelConfig model;

  external ffi.Pointer<ffi.Char> rule_fsts;

  @ffi.Int32()
  external int max_num_sentences;
}

final class SherpaOnnxGeneratedAudio extends ffi.Struct {
  /// in the range [-1, 1]
  external ffi.Pointer<ffi.Float> samples;

  /// number of samples
  @ffi.Int32()
  external int n;

  @ffi.Int32()
  external int sample_rate;
}

final class SherpaOnnxOfflineTts extends ffi.Opaque {}

typedef SherpaOnnxGeneratedAudioCallback
    = ffi.Pointer<ffi.NativeFunction<SherpaOnnxGeneratedAudioCallbackFunction>>;
typedef SherpaOnnxGeneratedAudioCallbackFunction = ffi.Void Function(
    ffi.Pointer<ffi.Float> samples, ffi.Int32 n);
typedef DartSherpaOnnxGeneratedAudioCallbackFunction = void Function(
    ffi.Pointer<ffi.Float> samples, int n);

const int LLAMA_DEFAULT_SEED = 4294967295;

const int LLAMA_MAX_RNG_STATE = 65536;

const int LLAMA_FILE_MAGIC_GGLA = 1734831201;

const int LLAMA_FILE_MAGIC_GGSN = 1734833006;

const int LLAMA_SESSION_MAGIC = 1734833006;

const int LLAMA_SESSION_VERSION = 4;
